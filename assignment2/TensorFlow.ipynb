{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's this TensorFlow business?\n",
    "\n",
    "You've written a lot of code in this assignment to provide a whole host of neural network functionality. Dropout, Batch Norm, and 2D convolutions are some of the workhorses of deep learning in computer vision. You've also worked hard to make your code efficient and vectorized.\n",
    "\n",
    "For the last part of this assignment, though, we're going to leave behind your beautiful codebase and instead migrate to one of two popular deep learning frameworks: in this instance, TensorFlow (or PyTorch, if you switch over to that notebook)\n",
    "\n",
    "#### What is it?\n",
    "TensorFlow is a system for executing computational graphs over Tensor objects, with native support for performing backpropogation for its Variables. In it, we work with Tensors which are n-dimensional arrays analogous to the numpy ndarray.\n",
    "\n",
    "#### Why?\n",
    "\n",
    "* Our code will now run on GPUs! Much faster training. Writing your own modules to run on GPUs is beyond the scope of this class, unfortunately.\n",
    "* We want you to be ready to use one of these frameworks for your project so you can experiment more efficiently than if you were writing every feature you want to use by hand. \n",
    "* We want you to stand on the shoulders of giants! TensorFlow and PyTorch are both excellent frameworks that will make your lives a lot easier, and now that you understand their guts, you are free to use them :) \n",
    "* We want you to be exposed to the sort of deep learning code you might run into in academia or industry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How will I learn TensorFlow?\n",
    "\n",
    "TensorFlow has many excellent tutorials available, including those from [Google themselves](https://www.tensorflow.org/get_started/get_started).\n",
    "\n",
    "Otherwise, this notebook will walk you through much of what you need to do to train models in TensorFlow. See the end of the notebook for some links to helpful tutorials if you want to learn more or need further clarification on topics that aren't fully explained here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.training import moving_averages\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from cs231n.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Model\n",
    "\n",
    "### Some useful utilities\n",
    "\n",
    ". Remember that our image data is initially N x H x W x C, where:\n",
    "* N is the number of datapoints\n",
    "* H is the height of each image in pixels\n",
    "* W is the height of each image in pixels\n",
    "* C is the number of channels (usually 3: R, G, B)\n",
    "\n",
    "This is the right way to represent the data when we are doing something like a 2D convolution, which needs spatial understanding of where the pixels are relative to each other. When we input image data into fully connected affine layers, however, we want each data example to be represented by a single vector -- it's no longer useful to segregate the different channels, rows, and columns of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The example model itself\n",
    "\n",
    "The first step to training your own model is defining its architecture.\n",
    "\n",
    "Here's an example of a convolutional neural network defined in TensorFlow -- try to understand what each line is doing, remembering that each layer is composed upon the previous layer. We haven't trained anything yet - that'll come next - for now, we want you to understand how everything gets set up. \n",
    "\n",
    "In that example, you see 2D convolutional layers (Conv2d), ReLU activations, and fully-connected layers (Linear). You also see the Hinge loss function, and the Adam optimizer being used. \n",
    "\n",
    "Make sure you understand why the parameters of the Linear layer are 5408 and 10.\n",
    "\n",
    "### TensorFlow Details\n",
    "In TensorFlow, much like in our previous notebooks, we'll first specifically initialize our variables, and then our network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# setup input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "def simple_model(X,y):\n",
    "    # define our weights (e.g. init_two_layer_convnet)\n",
    "    \n",
    "    # setup variables\n",
    "    Wconv1 = tf.get_variable(\"Wconv1\", shape=[7, 7, 3, 32])\n",
    "    bconv1 = tf.get_variable(\"bconv1\", shape=[32])\n",
    "    W1 = tf.get_variable(\"W1\", shape=[5408, 10])\n",
    "    b1 = tf.get_variable(\"b1\", shape=[10])\n",
    "\n",
    "    # define our graph (e.g. two_layer_convnet)\n",
    "    a1 = tf.nn.conv2d(X, Wconv1, strides=[1,2,2,1], padding='VALID') + bconv1\n",
    "    h1 = tf.nn.relu(a1)\n",
    "    h1_flat = tf.reshape(h1,[-1,5408])\n",
    "    y_out = tf.matmul(h1_flat,W1) + b1\n",
    "    return y_out\n",
    "\n",
    "y_out = simple_model(X,y)\n",
    "\n",
    "# define our loss\n",
    "total_loss = tf.losses.hinge_loss(tf.one_hot(y,10),logits=y_out)\n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "\n",
    "# define our optimizer\n",
    "optimizer = tf.train.AdamOptimizer(5e-4) # select optimizer and set learning rate\n",
    "train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow supports many other layer types, loss functions, and optimizers - you will experiment with these next. Here's the official API documentation for these (if any of the parameters used above were unclear, this resource will also be helpful). \n",
    "\n",
    "* Layers, Activations, Loss functions : https://www.tensorflow.org/api_guides/python/nn\n",
    "* Optimizers: https://www.tensorflow.org/api_guides/python/train#Optimizers\n",
    "* BatchNorm: https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model on one epoch\n",
    "While we have defined a graph of operations above, in order to execute TensorFlow Graphs, by feeding them input data and computing the results, we first need to create a `tf.Session` object. A session encapsulates the control and state of the TensorFlow runtime. For more information, see the TensorFlow [Getting started](https://www.tensorflow.org/get_started/get_started) guide.\n",
    "\n",
    "Optionally we can also specify a device context such as `/cpu:0` or `/gpu:0`. For documentation on this behavior see [this TensorFlow guide](https://www.tensorflow.org/tutorials/using_gpu)\n",
    "\n",
    "You should see a validation loss of around 0.4 to 0.6 and an accuracy of 0.30 to 0.35 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iteration 0: with minibatch training loss = 16.78 and accuracy of 0.1719\n",
      "Iteration 100: with minibatch training loss = 1.058 and accuracy of 0.2188\n",
      "Iteration 200: with minibatch training loss = 0.8991 and accuracy of 0.2969\n",
      "Iteration 300: with minibatch training loss = 0.6398 and accuracy of 0.3125\n",
      "Iteration 400: with minibatch training loss = 0.5932 and accuracy of 0.2656\n",
      "Iteration 500: with minibatch training loss = 0.6597 and accuracy of 0.3281\n",
      "Iteration 600: with minibatch training loss = 0.5186 and accuracy of 0.3438\n",
      "Iteration 700: with minibatch training loss = 0.4387 and accuracy of 0.4062\n",
      "Epoch 1, Overall loss = 0.806 and accuracy of 0.2903\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEWCAYAAACjYXoKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fef56a6bc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n",
      "Epoch 1, Overall loss = 0.4382 and accuracy of 0.368\n"
     ]
    }
   ],
   "source": [
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, plot_losses=False):\n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    # move shuffle to each epoch\n",
    "    # np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = training is not None\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss,correct_prediction,accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    \n",
    "    total_losses = []\n",
    "    total_accuracies = []\n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    for e in range(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        np.random.shuffle(train_indicies)\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.4g} and accuracy of {2:.4g}\"\\\n",
    "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.4g} and accuracy of {1:.4g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "        if plot_losses and epochs <= 20:\n",
    "            plt.plot(losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e+1))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "        total_losses.append(total_loss)\n",
    "        total_accuracies.append(total_correct)\n",
    "    if plot_losses and len(total_losses) > 1 and len(total_accuracies) > 1:\n",
    "        plt.grid(True)\n",
    "        plt.title('Result')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.ylim(0,3)\n",
    "        plt.plot(total_losses, label='loss')\n",
    "        plt.show()\n",
    "        plt.grid(True)\n",
    "        plt.title('Result')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.ylim(0.4,1)\n",
    "        plt.plot(total_accuracies, label='accuracy')\n",
    "        plt.show()\n",
    "    return total_loss,total_correct\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\" \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('Training')\n",
    "        run_model(sess,y_out,mean_loss,X_train,y_train,1,64,100,train_step,True)\n",
    "        print('Validation')\n",
    "        run_model(sess,y_out,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a specific model\n",
    "\n",
    "In this section, we're going to specify a model for you to construct. The goal here isn't to get good performance (that'll be next), but instead to get comfortable with understanding the TensorFlow documentation and configuring your own model. \n",
    "\n",
    "Using the code provided above as guidance, and using the following TensorFlow documentation, specify a model with the following architecture:\n",
    "\n",
    "* 7x7 Convolutional Layer with 32 filters and stride of 1\n",
    "* ReLU Activation Layer\n",
    "* Spatial Batch Normalization Layer (trainable parameters, with scale and centering)\n",
    "* 2x2 Max Pooling layer with a stride of 2\n",
    "* Affine layer with 1024 output units\n",
    "* ReLU Activation Layer\n",
    "* Affine layer from 1024 input units to 10 outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# define our input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "# define model\n",
    "def complex_model(X,y,is_training):\n",
    "    Wconv1 = tf.get_variable(\"Wconv1\", shape=[7, 7, 3, 32],\n",
    "                             initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    bconv1 = tf.get_variable(\"bconv1\", shape=[32], initializer=tf.constant_initializer(0.1))\n",
    "    moving_mean1 = tf.get_variable('moving_mean1', shape=[32], initializer=tf.zeros_initializer, trainable=False)\n",
    "    moving_var1 = tf.get_variable('moving_var1', shape=[32], initializer=tf.ones_initializer, trainable=False)\n",
    "    gamma1 = tf.get_variable(\"gamma1\", shape=[32], initializer=tf.ones_initializer)\n",
    "    beta1 = tf.get_variable(\"beta1\", shape=[32], initializer=tf.zeros_initializer)\n",
    "    W2 = tf.get_variable(\"W2\", shape=[5408, 1024],\n",
    "                             initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    b2 = tf.get_variable(\"b2\", shape=[1024], initializer=tf.constant_initializer(0.1))\n",
    "    W3 = tf.get_variable(\"W3\", shape=[1024, 10],\n",
    "                             initializer=tf.truncated_normal_initializer(stddev=0.01))\n",
    "    b3 = tf.get_variable(\"b3\", shape=[10], initializer=tf.zeros_initializer)\n",
    "\n",
    "    conv1 = tf.nn.conv2d(X, Wconv1, strides=[1,1,1,1], padding='VALID') + bconv1\n",
    "    relu1 = tf.nn.relu(conv1)\n",
    "\n",
    "    mean, variance = tf.nn.moments(conv1, axes=[0,1,2])\n",
    "    update_moving_mean = moving_averages.assign_moving_average(moving_mean1, mean, 0.99)\n",
    "    update_moving_variance = moving_averages.assign_moving_average(moving_var1, variance, 0.99)\n",
    "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_moving_mean)\n",
    "    tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_moving_variance)\n",
    "    mean1, var1 = control_flow_ops.cond(is_training, lambda: (mean, variance), lambda: (moving_mean1, moving_var1))\n",
    "\n",
    "    bn1 = tf.nn.batch_normalization(relu1, mean=mean1, variance=var1, offset=beta1, scale=gamma1, variance_epsilon=1e-5)\n",
    "    pool1 = tf.nn.max_pool(bn1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    flat1 = tf.reshape(pool1,[-1,5408])\n",
    "    fc2 = tf.matmul(flat1,W2) + b2\n",
    "    relu2 = tf.nn.relu(fc2)\n",
    "    y_out = tf.matmul(relu2,W3) + b3\n",
    "    return y_out\n",
    "\n",
    "y_out = complex_model(X,y,is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure you're doing the right thing, use the following tool to check the dimensionality of your output (it should be 64 x 10, since our batches have size 64 and the output of the final affine layer should be 10, corresponding to our 10 classes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.53 ms ± 30.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "(64, 10)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Now we're going to feed a random batch into the model \n",
    "# and make sure the output is the right size\n",
    "x = np.random.randn(64, 32, 32,3)\n",
    "with tf.Session(config=config) as sess:\n",
    "    with tf.device(\"/cpu:0\"): #\"/cpu:0\" or \"/gpu:0\"\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        ans = sess.run(y_out,feed_dict={X:x,is_training:True})\n",
    "        %timeit sess.run(y_out,feed_dict={X:x,is_training:True})\n",
    "        print(ans.shape)\n",
    "        print(np.array_equal(ans.shape, np.array([64, 10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the following from the run above \n",
    "\n",
    "`(64, 10)`\n",
    "\n",
    "`True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU!\n",
    "\n",
    "Now, we're going to try and start the model under the GPU device, the rest of the code stays unchanged and all our variables and operations will be computed using accelerated code paths. However, if there is no GPU, we get a Python exception and have to rebuild our graph. On a dual-core CPU, you might see around 50-80ms/batch running the above, while the Google Cloud GPUs (run below) should be around 2-5ms/batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.54 ms ± 24.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with tf.Session(config=config) as sess:\n",
    "        with tf.device(\"/gpu:0\") as dev: #\"/cpu:0\" or \"/gpu:0\"\n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "            ans = sess.run(y_out,feed_dict={X:x,is_training:True})\n",
    "            %timeit sess.run(y_out,feed_dict={X:x,is_training:True})\n",
    "except tf.errors.InvalidArgumentError:\n",
    "    print(\"no gpu found, please use Google Cloud if you want GPU acceleration\")    \n",
    "    # rebuild the graph\n",
    "    # trying to start a GPU throws an exception \n",
    "    # and also trashes the original graph\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    y = tf.placeholder(tf.int64, [None])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    y_out = complex_model(X,y,is_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should observe that even a simple forward pass like this is significantly faster on the GPU. So for the rest of the assignment (and when you go train your models in assignment 3 and your project!), you should use GPU devices. However, with TensorFlow, the default device is a GPU if one is available, and a CPU otherwise, so we can skip the device specification from now on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model.\n",
    "\n",
    "Now that you've seen how to define a model and do a single forward pass of some data through it, let's  walk through how you'd actually train one whole epoch over your training data (using the complex_model you created provided above).\n",
    "\n",
    "Make sure you understand how each TensorFlow function used below corresponds to what you implemented in your custom neural network implementation.\n",
    "\n",
    "First, set up an **RMSprop optimizer** (using a 1e-3 learning rate) and a **cross-entropy loss** function. See the TensorFlow documentation for more information\n",
    "* Layers, Activations, Loss functions : https://www.tensorflow.org/api_guides/python/nn\n",
    "* Optimizers: https://www.tensorflow.org/api_guides/python/train#Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "#     y_out: is what your model computes\n",
    "#     y: is your TensorFlow variable with label information\n",
    "# Outputs\n",
    "#    mean_loss: a TensorFlow variable (scalar) with numerical loss\n",
    "#    optimizer: a TensorFlow optimizer\n",
    "# This should be ~3 lines of code!\n",
    "mean_loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(tf.one_hot(y,10),logits=y_out))\n",
    "optimizer = tf.train.RMSPropOptimizer(1e-3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Below we'll create a session and train the model over one epoch. You should see a loss of 1.4 to 2.0 and an accuracy of 0.4 to 0.5. There will be some variation due to random seeds and differences in initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iteration 0: with minibatch training loss = 2.314 and accuracy of 0.125\n",
      "Iteration 100: with minibatch training loss = 2.052 and accuracy of 0.1875\n",
      "Iteration 200: with minibatch training loss = 1.995 and accuracy of 0.25\n",
      "Iteration 300: with minibatch training loss = 1.834 and accuracy of 0.375\n",
      "Iteration 400: with minibatch training loss = 1.343 and accuracy of 0.5156\n",
      "Iteration 500: with minibatch training loss = 1.414 and accuracy of 0.5\n",
      "Iteration 600: with minibatch training loss = 1.388 and accuracy of 0.4531\n",
      "Iteration 700: with minibatch training loss = 1.361 and accuracy of 0.5\n",
      "Epoch 1, Overall loss = 1.616 and accuracy of 0.4222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.6163608194078718, 0.42222448979591837)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session(config=config)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,1,64,100,train_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the accuracy of the model.\n",
    "\n",
    "Let's see the train and test code in action -- feel free to use these methods when evaluating the models you develop below. You should see a loss of 1.3 to 2.0 with an accuracy of 0.45 to 0.55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n",
      "Epoch 1, Overall loss = 1.295 and accuracy of 0.534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.2950451412200927, 0.53400000000000003)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Validation')\n",
    "run_model(sess,y_out,mean_loss,X_val,y_val,1,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a _great_ model on CIFAR-10!\n",
    "\n",
    "Now it's your job to experiment with architectures, hyperparameters, loss functions, and optimizers to train a model that achieves ** >= 70% accuracy on the validation set** of CIFAR-10. You can use the `run_model` function from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things you should try:\n",
    "- **Filter size**: Above we used 7x7; this makes pretty pictures but smaller filters may be more efficient\n",
    "- **Number of filters**: Above we used 32 filters. Do more or fewer do better?\n",
    "- **Pooling vs Strided Convolution**: Do you use max pooling or just stride convolutions?\n",
    "- **Batch normalization**: Try adding spatial batch normalization after convolution layers and vanilla batch normalization after affine layers. Do your networks train faster?\n",
    "- **Network architecture**: The network above has two layers of trainable parameters. Can you do better with a deep network? Good architectures to try include:\n",
    "    - [conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [batchnorm-relu-conv]xN -> [affine]xM -> [softmax or SVM]\n",
    "- **Use TensorFlow Scope**: Use TensorFlow scope and/or [tf.layers](https://www.tensorflow.org/api_docs/python/tf/layers) to make it easier to write deeper networks. See [this tutorial](https://www.tensorflow.org/tutorials/layers) for how to use `tf.layers`. \n",
    "- **Use Learning Rate Decay**: [As the notes point out](http://cs231n.github.io/neural-networks-3/#anneal), decaying the learning rate might help the model converge. Feel free to decay every epoch, when loss doesn't change over an entire epoch, or any other heuristic you find appropriate. See the [Tensorflow documentation](https://www.tensorflow.org/versions/master/api_guides/python/train#Decaying_the_learning_rate) for learning rate decay.\n",
    "- **Global Average Pooling**: Instead of flattening and then having multiple affine layers, perform convolutions until your image gets small (7x7 or so) and then perform an average pooling operation to get to a 1x1 image picture (1, 1 , Filter#), which is then reshaped into a (Filter#) vector. This is used in [Google's Inception Network](https://arxiv.org/abs/1512.00567) (See Table 1 for their architecture).\n",
    "- **Regularization**: Add l2 weight regularization, or perhaps use [Dropout as in the TensorFlow MNIST tutorial](https://www.tensorflow.org/get_started/mnist/pros)\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and regularization strength. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "- You should use the validation set for hyperparameter search, and we'll save the test set for evaluating your architecture on the best parameters as selected by the validation set.\n",
    "\n",
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these; however they would be good things to try for extra credit.\n",
    "\n",
    "- Alternative update steps: For the assignment we implemented SGD+momentum, RMSprop, and Adam; you could try alternatives like AdaGrad or AdaDelta.\n",
    "- Alternative activation functions such as leaky ReLU, parametric ReLU, ELU, or MaxOut.\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "- New Architectures\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385) where the input from the previous layer is added to the output.\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993) where inputs into previous layers are concatenated together.\n",
    "  - [This blog has an in-depth overview](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n",
    "\n",
    "If you do decide to implement something extra, clearly describe it in the \"Extra Credit Description\" cell below.\n",
    "\n",
    "### What we expect\n",
    "At the very least, you should be able to train a ConvNet that gets at **>= 70% accuracy on the validation set**. This is just a lower bound - if you are careful it should be possible to get accuracies much higher than that! Extra credit points will be awarded for particularly high-scoring models or unique approaches.\n",
    "\n",
    "You should use the space below to experiment and train your network. The final cell in this notebook should contain the training and validation set accuracies for your final trained network.\n",
    "\n",
    "Have fun and happy training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feel free to play with this cell\n",
    "\n",
    "def conv(X, name, shape, strides=[1,1,1,1], padding='SAME', act=tf.nn.relu, use_bn=True, is_training=False, weight_decay=0.0005):\n",
    "    out = None\n",
    "    with tf.variable_scope(\"scope\"):\n",
    "        W = tf.get_variable(\"%s/W\"%name, shape=shape,\n",
    "                            initializer=tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode='FAN_IN', uniform=False),\n",
    "                            regularizer=tf.contrib.layers.l2_regularizer(weight_decay))\n",
    "        if not use_bn:\n",
    "            b = tf.get_variable(\"%s/b\"%name, shape=(shape[-1]), initializer=tf.constant_initializer(0.1))\n",
    "        if use_bn:\n",
    "            global_mean = tf.get_variable(\"%s/global_mean\"%name, shape=(shape[-1]), initializer=tf.zeros_initializer, trainable=False)\n",
    "            global_var = tf.get_variable(\"%s/global_var\"%name, shape=(shape[-1]), initializer=tf.ones_initializer, trainable=False)\n",
    "            gamma = tf.get_variable(\"%s/gamma\"%name, shape=(shape[-1]), initializer=tf.ones_initializer)\n",
    "            beta = tf.get_variable(\"%s/beta\"%name, shape=(shape[-1]), initializer=tf.zeros_initializer)\n",
    "\n",
    "        conv = tf.nn.conv2d(X, W, strides=strides, padding=padding)\n",
    "        if not use_bn:\n",
    "            conv = conv + b\n",
    "        if act==None:\n",
    "            act = tf.identity\n",
    "        if use_bn:\n",
    "            mean, var = tf.nn.moments(conv, axes=[0,1,2])\n",
    "            update_moving_mean = moving_averages.assign_moving_average(global_mean, mean, 0.999)\n",
    "            update_moving_var = moving_averages.assign_moving_average(global_var, var, 0.999)\n",
    "            tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_moving_mean)\n",
    "            tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_moving_var)\n",
    "            mean, vare = control_flow_ops.cond(is_training, lambda: (mean, var), lambda: (global_mean, global_var))\n",
    "            bn = tf.nn.batch_normalization(conv, mean=mean, variance=var, offset=beta, scale=gamma, variance_epsilon=1e-5)\n",
    "            out = act(bn)\n",
    "        else:\n",
    "            out = act(conv)\n",
    "    return out\n",
    "\n",
    "def fc(X, name, shape, act=tf.nn.relu, use_bn=True, is_training=False, weight_decay=0.0005):\n",
    "    out = None\n",
    "    with tf.variable_scope(\"scope\"):\n",
    "        W = tf.get_variable(\"%s/W\"%name, shape=shape,\n",
    "                            initializer=tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode='FAN_IN', uniform=False),\n",
    "                            regularizer=tf.contrib.layers.l2_regularizer(weight_decay))\n",
    "        if not use_bn:\n",
    "            b = tf.get_variable(\"%s/b\"%name, shape=(shape[-1]), initializer=tf.constant_initializer(0.1))\n",
    "        if use_bn:\n",
    "            global_mean = tf.get_variable(\"%s/global_mean\"%name, shape=(shape[-1]), initializer=tf.zeros_initializer, trainable=False)\n",
    "            global_var = tf.get_variable(\"%s/global_var\"%name, shape=(shape[-1]), initializer=tf.ones_initializer, trainable=False)\n",
    "            gamma = tf.get_variable(\"%s/gamma\"%name, shape=(shape[-1]), initializer=tf.ones_initializer)\n",
    "            beta = tf.get_variable(\"%s/beta\"%name, shape=(shape[-1]), initializer=tf.zeros_initializer)\n",
    "\n",
    "        fc = tf.matmul(X,W)\n",
    "        if not use_bn:\n",
    "            fc = fc + b\n",
    "        if act==None:\n",
    "            act = tf.identity\n",
    "        if use_bn:\n",
    "            mean, var = tf.nn.moments(fc, axes=[0])\n",
    "            update_moving_mean = moving_averages.assign_moving_average(global_mean, mean, 0.999)\n",
    "            update_moving_var = moving_averages.assign_moving_average(global_var, var, 0.999)\n",
    "            tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_moving_mean)\n",
    "            tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, update_moving_var)\n",
    "            mean, vare = control_flow_ops.cond(is_training, lambda: (mean, var), lambda: (global_mean, global_var))\n",
    "            bn = tf.nn.batch_normalization(fc, mean=mean, variance=var, offset=beta, scale=gamma, variance_epsilon=1e-5)\n",
    "            out = act(bn)\n",
    "        else:\n",
    "            out = act(fc)\n",
    "    return out\n",
    "\n",
    "def preprocess(X):\n",
    "    shape = tf.shape(X)\n",
    "    batch_size, height, width, channel = shape[0], shape[1], shape[2], shape[3]\n",
    "    image = tf.transpose(X, [1, 2, 3, 0])\n",
    "    image = tf.reshape(image, [height, width, channel*batch_size])\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.reshape(image, [height, width, channel, batch_size])\n",
    "    image = tf.transpose(image, [3, 0, 1, 2])\n",
    "    image = tf.random_crop(image, size=[batch_size, 28, 28, channel])\n",
    "    image = tf.image.resize_images(image, (height, width))\n",
    "    return image\n",
    "\n",
    "def my_model(X,y,is_training):\n",
    "    image = control_flow_ops.cond(is_training, lambda: preprocess(X), lambda: X)\n",
    "    image = image / 127.5\n",
    "    conv1 = conv(image, name=\"conv1\", shape=[3,3,3,32], strides=[1,1,1,1], padding='SAME', act=tf.nn.leaky_relu, use_bn=True, is_training=is_training)\n",
    "    conv2 = conv(conv1, name=\"conv2\", shape=[3,3,32,32], strides=[1,2,2,1], padding='SAME', act=tf.nn.leaky_relu, use_bn=True, is_training=is_training)\n",
    "    conv3 = conv(conv2, name=\"conv3\", shape=[3,3,32,64], strides=[1,1,1,1], padding='SAME', act=tf.nn.leaky_relu, use_bn=True, is_training=is_training)\n",
    "    conv4 = conv(conv3, name=\"conv4\", shape=[3,3,64,64], strides=[1,2,2,1], padding='SAME', act=tf.nn.leaky_relu, use_bn=True, is_training=is_training)\n",
    "    conv5 = conv(conv4, name=\"conv5\", shape=[3,3,64,128], strides=[1,1,1,1], padding='SAME', act=tf.nn.leaky_relu, use_bn=True, is_training=is_training)\n",
    "    conv6 = conv(conv5, name=\"conv6\", shape=[3,3,128,128], strides=[1,2,2,1], padding='SAME', act=tf.nn.leaky_relu, use_bn=True, is_training=is_training)\n",
    "    conv7 = conv(conv6, name=\"conv7\", shape=[3,3,128,256], strides=[1,1,1,1], padding='SAME', act=tf.nn.leaky_relu, use_bn=True, is_training=is_training)\n",
    "    conv8 = conv(conv7, name=\"conv8\", shape=[3,3,256,256], strides=[1,1,1,1], padding='SAME', act=tf.nn.leaky_relu, use_bn=True, is_training=is_training)\n",
    "    conv9 = conv(conv8, name=\"conv9\", shape=[1,1,256,10], strides=[1,1,1,1], padding='SAME', act=tf.identity, use_bn=False, is_training=is_training)\n",
    "    pool9 = tf.nn.avg_pool(conv9, ksize=[1,4,4,1], strides=[1,4,4,1], padding='VALID')\n",
    "    y_out = tf.reshape(pool9,[-1,10])\n",
    "    return y_out\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "y_out = my_model(X,y,is_training)\n",
    "regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "cross_entropy_mean = tf.reduce_mean(tf.losses.softmax_cross_entropy(tf.one_hot(y,10),logits=y_out))\n",
    "mean_loss = tf.add_n([cross_entropy_mean] + regularization_losses)\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "starter_learning_rate = 1e-1\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           49000, 0.1, staircase=True)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "\n",
    "# batch normalization in tensorflow requires this extra dependency\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    train_step = optimizer.minimize(mean_loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iteration 0: with minibatch training loss = 2.963 and accuracy of 0.1\n",
      "Epoch 1, Overall loss = 1.911 and accuracy of 0.4621\n",
      "Iteration 490: with minibatch training loss = 1.716 and accuracy of 0.57\n",
      "Epoch 2, Overall loss = 1.439 and accuracy of 0.6103\n",
      "Iteration 980: with minibatch training loss = 1.354 and accuracy of 0.57\n",
      "Epoch 3, Overall loss = 1.218 and accuracy of 0.6738\n",
      "Iteration 1470: with minibatch training loss = 1.052 and accuracy of 0.68\n",
      "Epoch 4, Overall loss = 1.106 and accuracy of 0.7075\n",
      "Iteration 1960: with minibatch training loss = 1.056 and accuracy of 0.71\n",
      "Epoch 5, Overall loss = 1.031 and accuracy of 0.7293\n",
      "Iteration 2450: with minibatch training loss = 0.8495 and accuracy of 0.81\n",
      "Epoch 6, Overall loss = 0.9961 and accuracy of 0.7374\n",
      "Iteration 2940: with minibatch training loss = 1.142 and accuracy of 0.72\n",
      "Epoch 7, Overall loss = 0.9642 and accuracy of 0.7519\n",
      "Iteration 3430: with minibatch training loss = 0.8972 and accuracy of 0.77\n",
      "Epoch 8, Overall loss = 0.9484 and accuracy of 0.756\n",
      "Iteration 3920: with minibatch training loss = 0.8009 and accuracy of 0.77\n",
      "Epoch 9, Overall loss = 0.9294 and accuracy of 0.7644\n",
      "Iteration 4410: with minibatch training loss = 0.8461 and accuracy of 0.8\n",
      "Epoch 10, Overall loss = 0.9208 and accuracy of 0.7682\n",
      "Iteration 4900: with minibatch training loss = 0.9282 and accuracy of 0.71\n",
      "Epoch 11, Overall loss = 0.9076 and accuracy of 0.7745\n",
      "Iteration 5390: with minibatch training loss = 1.116 and accuracy of 0.69\n",
      "Epoch 12, Overall loss = 0.9103 and accuracy of 0.7758\n",
      "Iteration 5880: with minibatch training loss = 0.8955 and accuracy of 0.75\n",
      "Epoch 13, Overall loss = 0.8983 and accuracy of 0.7804\n",
      "Iteration 6370: with minibatch training loss = 1.065 and accuracy of 0.75\n",
      "Epoch 14, Overall loss = 0.8977 and accuracy of 0.7838\n",
      "Iteration 6860: with minibatch training loss = 0.8145 and accuracy of 0.84\n",
      "Epoch 15, Overall loss = 0.8961 and accuracy of 0.7854\n",
      "Iteration 7350: with minibatch training loss = 0.7489 and accuracy of 0.84\n",
      "Epoch 16, Overall loss = 0.8952 and accuracy of 0.7881\n",
      "Iteration 7840: with minibatch training loss = 0.8943 and accuracy of 0.79\n",
      "Epoch 17, Overall loss = 0.8823 and accuracy of 0.7915\n",
      "Iteration 8330: with minibatch training loss = 0.9307 and accuracy of 0.77\n",
      "Epoch 18, Overall loss = 0.8856 and accuracy of 0.7947\n",
      "Iteration 8820: with minibatch training loss = 0.7981 and accuracy of 0.81\n",
      "Epoch 19, Overall loss = 0.8848 and accuracy of 0.7942\n",
      "Iteration 9310: with minibatch training loss = 0.8116 and accuracy of 0.8\n",
      "Epoch 20, Overall loss = 0.8824 and accuracy of 0.796\n",
      "Iteration 9800: with minibatch training loss = 0.8212 and accuracy of 0.83\n",
      "Epoch 21, Overall loss = 0.8833 and accuracy of 0.7965\n",
      "Iteration 10290: with minibatch training loss = 0.871 and accuracy of 0.83\n",
      "Epoch 22, Overall loss = 0.8759 and accuracy of 0.7997\n",
      "Iteration 10780: with minibatch training loss = 0.7964 and accuracy of 0.86\n",
      "Epoch 23, Overall loss = 0.882 and accuracy of 0.7968\n",
      "Iteration 11270: with minibatch training loss = 0.8219 and accuracy of 0.8\n",
      "Epoch 24, Overall loss = 0.8773 and accuracy of 0.8017\n",
      "Iteration 11760: with minibatch training loss = 0.9896 and accuracy of 0.75\n",
      "Epoch 25, Overall loss = 0.8714 and accuracy of 0.8035\n",
      "Iteration 12250: with minibatch training loss = 0.9221 and accuracy of 0.8\n",
      "Epoch 26, Overall loss = 0.8778 and accuracy of 0.8007\n",
      "Iteration 12740: with minibatch training loss = 0.9443 and accuracy of 0.79\n",
      "Epoch 27, Overall loss = 0.881 and accuracy of 0.8007\n",
      "Iteration 13230: with minibatch training loss = 0.8951 and accuracy of 0.83\n",
      "Epoch 28, Overall loss = 0.8732 and accuracy of 0.8036\n",
      "Iteration 13720: with minibatch training loss = 1.015 and accuracy of 0.76\n",
      "Epoch 29, Overall loss = 0.8755 and accuracy of 0.8028\n",
      "Iteration 14210: with minibatch training loss = 0.7886 and accuracy of 0.76\n",
      "Epoch 30, Overall loss = 0.8781 and accuracy of 0.8031\n",
      "Iteration 14700: with minibatch training loss = 0.9693 and accuracy of 0.79\n",
      "Epoch 31, Overall loss = 0.8712 and accuracy of 0.8052\n",
      "Iteration 15190: with minibatch training loss = 0.7163 and accuracy of 0.85\n",
      "Epoch 32, Overall loss = 0.8703 and accuracy of 0.8066\n",
      "Iteration 15680: with minibatch training loss = 0.7897 and accuracy of 0.82\n",
      "Epoch 33, Overall loss = 0.871 and accuracy of 0.8062\n",
      "Iteration 16170: with minibatch training loss = 0.7865 and accuracy of 0.86\n",
      "Epoch 34, Overall loss = 0.8686 and accuracy of 0.8084\n",
      "Iteration 16660: with minibatch training loss = 0.8384 and accuracy of 0.83\n",
      "Epoch 35, Overall loss = 0.8737 and accuracy of 0.8074\n",
      "Iteration 17150: with minibatch training loss = 0.8315 and accuracy of 0.82\n",
      "Epoch 36, Overall loss = 0.8717 and accuracy of 0.8079\n",
      "Iteration 17640: with minibatch training loss = 0.8028 and accuracy of 0.87\n",
      "Epoch 37, Overall loss = 0.8723 and accuracy of 0.8056\n",
      "Iteration 18130: with minibatch training loss = 0.8593 and accuracy of 0.82\n",
      "Epoch 38, Overall loss = 0.8712 and accuracy of 0.8082\n",
      "Iteration 18620: with minibatch training loss = 0.6898 and accuracy of 0.89\n",
      "Epoch 39, Overall loss = 0.8689 and accuracy of 0.8099\n",
      "Iteration 19110: with minibatch training loss = 0.8954 and accuracy of 0.78\n",
      "Epoch 40, Overall loss = 0.8651 and accuracy of 0.8096\n",
      "Iteration 19600: with minibatch training loss = 0.9241 and accuracy of 0.81\n",
      "Epoch 41, Overall loss = 0.8725 and accuracy of 0.8078\n",
      "Iteration 20090: with minibatch training loss = 0.8067 and accuracy of 0.82\n",
      "Epoch 42, Overall loss = 0.8648 and accuracy of 0.8106\n",
      "Iteration 20580: with minibatch training loss = 0.7681 and accuracy of 0.84\n",
      "Epoch 43, Overall loss = 0.8731 and accuracy of 0.8105\n",
      "Iteration 21070: with minibatch training loss = 0.879 and accuracy of 0.83\n",
      "Epoch 44, Overall loss = 0.8679 and accuracy of 0.812\n",
      "Iteration 21560: with minibatch training loss = 0.8482 and accuracy of 0.8\n",
      "Epoch 45, Overall loss = 0.8701 and accuracy of 0.8106\n",
      "Iteration 22050: with minibatch training loss = 0.7469 and accuracy of 0.85\n",
      "Epoch 46, Overall loss = 0.863 and accuracy of 0.8137\n",
      "Iteration 22540: with minibatch training loss = 0.8903 and accuracy of 0.8\n",
      "Epoch 47, Overall loss = 0.8628 and accuracy of 0.8146\n",
      "Iteration 23030: with minibatch training loss = 0.8502 and accuracy of 0.83\n",
      "Epoch 48, Overall loss = 0.865 and accuracy of 0.813\n",
      "Iteration 23520: with minibatch training loss = 0.8161 and accuracy of 0.81\n",
      "Epoch 49, Overall loss = 0.8706 and accuracy of 0.8122\n",
      "Iteration 24010: with minibatch training loss = 0.7925 and accuracy of 0.85\n",
      "Epoch 50, Overall loss = 0.869 and accuracy of 0.8111\n",
      "Iteration 24500: with minibatch training loss = 0.7489 and accuracy of 0.84\n",
      "Epoch 51, Overall loss = 0.862 and accuracy of 0.8131\n",
      "Iteration 24990: with minibatch training loss = 0.78 and accuracy of 0.84\n",
      "Epoch 52, Overall loss = 0.8681 and accuracy of 0.8126\n",
      "Iteration 25480: with minibatch training loss = 0.8494 and accuracy of 0.81\n",
      "Epoch 53, Overall loss = 0.8642 and accuracy of 0.8131\n",
      "Iteration 25970: with minibatch training loss = 0.7321 and accuracy of 0.85\n",
      "Epoch 54, Overall loss = 0.8665 and accuracy of 0.8129\n",
      "Iteration 26460: with minibatch training loss = 0.8797 and accuracy of 0.81\n",
      "Epoch 55, Overall loss = 0.8649 and accuracy of 0.8125\n",
      "Iteration 26950: with minibatch training loss = 0.9901 and accuracy of 0.81\n",
      "Epoch 56, Overall loss = 0.8602 and accuracy of 0.815\n",
      "Iteration 27440: with minibatch training loss = 0.834 and accuracy of 0.82\n",
      "Epoch 57, Overall loss = 0.8693 and accuracy of 0.8139\n",
      "Iteration 27930: with minibatch training loss = 0.778 and accuracy of 0.85\n",
      "Epoch 58, Overall loss = 0.8633 and accuracy of 0.8131\n",
      "Iteration 28420: with minibatch training loss = 0.8837 and accuracy of 0.84\n",
      "Epoch 59, Overall loss = 0.8627 and accuracy of 0.8141\n",
      "Iteration 28910: with minibatch training loss = 1.041 and accuracy of 0.76\n",
      "Epoch 60, Overall loss = 0.8604 and accuracy of 0.8151\n",
      "Iteration 29400: with minibatch training loss = 0.8532 and accuracy of 0.83\n",
      "Epoch 61, Overall loss = 0.861 and accuracy of 0.8158\n",
      "Iteration 29890: with minibatch training loss = 0.8112 and accuracy of 0.79\n",
      "Epoch 62, Overall loss = 0.8649 and accuracy of 0.8128\n",
      "Iteration 30380: with minibatch training loss = 1.032 and accuracy of 0.75\n",
      "Epoch 63, Overall loss = 0.861 and accuracy of 0.8173\n",
      "Iteration 30870: with minibatch training loss = 0.8769 and accuracy of 0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64, Overall loss = 0.8556 and accuracy of 0.8205\n",
      "Iteration 31360: with minibatch training loss = 0.9036 and accuracy of 0.81\n",
      "Epoch 65, Overall loss = 0.8637 and accuracy of 0.815\n",
      "Iteration 31850: with minibatch training loss = 0.7696 and accuracy of 0.84\n",
      "Epoch 66, Overall loss = 0.8559 and accuracy of 0.8179\n",
      "Iteration 32340: with minibatch training loss = 0.7396 and accuracy of 0.85\n",
      "Epoch 67, Overall loss = 0.8615 and accuracy of 0.8167\n",
      "Iteration 32830: with minibatch training loss = 0.8693 and accuracy of 0.8\n",
      "Epoch 68, Overall loss = 0.8654 and accuracy of 0.8149\n",
      "Iteration 33320: with minibatch training loss = 0.9508 and accuracy of 0.76\n",
      "Epoch 69, Overall loss = 0.8594 and accuracy of 0.8174\n",
      "Iteration 33810: with minibatch training loss = 0.9478 and accuracy of 0.79\n",
      "Epoch 70, Overall loss = 0.8644 and accuracy of 0.8153\n",
      "Iteration 34300: with minibatch training loss = 1.026 and accuracy of 0.76\n",
      "Epoch 71, Overall loss = 0.8641 and accuracy of 0.8157\n",
      "Iteration 34790: with minibatch training loss = 0.9275 and accuracy of 0.8\n",
      "Epoch 72, Overall loss = 0.8584 and accuracy of 0.8193\n",
      "Iteration 35280: with minibatch training loss = 0.9367 and accuracy of 0.79\n",
      "Epoch 73, Overall loss = 0.8636 and accuracy of 0.814\n",
      "Iteration 35770: with minibatch training loss = 0.809 and accuracy of 0.82\n",
      "Epoch 74, Overall loss = 0.859 and accuracy of 0.8169\n",
      "Iteration 36260: with minibatch training loss = 0.8551 and accuracy of 0.81\n",
      "Epoch 75, Overall loss = 0.8647 and accuracy of 0.8172\n",
      "Iteration 36750: with minibatch training loss = 0.6921 and accuracy of 0.88\n",
      "Epoch 76, Overall loss = 0.8671 and accuracy of 0.8155\n",
      "Iteration 37240: with minibatch training loss = 0.8123 and accuracy of 0.88\n",
      "Epoch 77, Overall loss = 0.8625 and accuracy of 0.8182\n",
      "Iteration 37730: with minibatch training loss = 0.7103 and accuracy of 0.85\n",
      "Epoch 78, Overall loss = 0.8618 and accuracy of 0.8167\n",
      "Iteration 38220: with minibatch training loss = 0.7667 and accuracy of 0.83\n",
      "Epoch 79, Overall loss = 0.8574 and accuracy of 0.8182\n",
      "Iteration 38710: with minibatch training loss = 0.8537 and accuracy of 0.85\n",
      "Epoch 80, Overall loss = 0.8603 and accuracy of 0.8186\n",
      "Iteration 39200: with minibatch training loss = 0.7143 and accuracy of 0.87\n",
      "Epoch 81, Overall loss = 0.8561 and accuracy of 0.8189\n",
      "Iteration 39690: with minibatch training loss = 0.7656 and accuracy of 0.84\n",
      "Epoch 82, Overall loss = 0.8599 and accuracy of 0.8171\n",
      "Iteration 40180: with minibatch training loss = 0.8727 and accuracy of 0.81\n",
      "Epoch 83, Overall loss = 0.8615 and accuracy of 0.8191\n",
      "Iteration 40670: with minibatch training loss = 0.8418 and accuracy of 0.79\n",
      "Epoch 84, Overall loss = 0.8626 and accuracy of 0.8184\n",
      "Iteration 41160: with minibatch training loss = 0.8082 and accuracy of 0.85\n",
      "Epoch 85, Overall loss = 0.8674 and accuracy of 0.8181\n",
      "Iteration 41650: with minibatch training loss = 0.8885 and accuracy of 0.82\n",
      "Epoch 86, Overall loss = 0.8575 and accuracy of 0.8199\n",
      "Iteration 42140: with minibatch training loss = 0.9024 and accuracy of 0.74\n",
      "Epoch 87, Overall loss = 0.8562 and accuracy of 0.819\n",
      "Iteration 42630: with minibatch training loss = 0.7428 and accuracy of 0.85\n",
      "Epoch 88, Overall loss = 0.8612 and accuracy of 0.8186\n",
      "Iteration 43120: with minibatch training loss = 0.8666 and accuracy of 0.79\n",
      "Epoch 89, Overall loss = 0.8641 and accuracy of 0.8196\n",
      "Iteration 43610: with minibatch training loss = 0.8274 and accuracy of 0.85\n",
      "Epoch 90, Overall loss = 0.8615 and accuracy of 0.8185\n",
      "Iteration 44100: with minibatch training loss = 0.7861 and accuracy of 0.83\n",
      "Epoch 91, Overall loss = 0.8585 and accuracy of 0.8198\n",
      "Iteration 44590: with minibatch training loss = 0.8595 and accuracy of 0.85\n",
      "Epoch 92, Overall loss = 0.8644 and accuracy of 0.8181\n",
      "Iteration 45080: with minibatch training loss = 0.7924 and accuracy of 0.84\n",
      "Epoch 93, Overall loss = 0.8606 and accuracy of 0.8188\n",
      "Iteration 45570: with minibatch training loss = 0.8117 and accuracy of 0.87\n",
      "Epoch 94, Overall loss = 0.8668 and accuracy of 0.8161\n",
      "Iteration 46060: with minibatch training loss = 0.8534 and accuracy of 0.79\n",
      "Epoch 95, Overall loss = 0.8608 and accuracy of 0.8176\n",
      "Iteration 46550: with minibatch training loss = 0.8569 and accuracy of 0.82\n",
      "Epoch 96, Overall loss = 0.8624 and accuracy of 0.818\n",
      "Iteration 47040: with minibatch training loss = 0.8364 and accuracy of 0.82\n",
      "Epoch 97, Overall loss = 0.8585 and accuracy of 0.8206\n",
      "Iteration 47530: with minibatch training loss = 0.8645 and accuracy of 0.82\n",
      "Epoch 98, Overall loss = 0.8552 and accuracy of 0.8198\n",
      "Iteration 48020: with minibatch training loss = 1.065 and accuracy of 0.73\n",
      "Epoch 99, Overall loss = 0.8643 and accuracy of 0.8177\n",
      "Iteration 48510: with minibatch training loss = 0.8022 and accuracy of 0.84\n",
      "Epoch 100, Overall loss = 0.8579 and accuracy of 0.82\n",
      "Iteration 49000: with minibatch training loss = 0.7251 and accuracy of 0.88\n",
      "Epoch 101, Overall loss = 0.695 and accuracy of 0.8763\n",
      "Iteration 49490: with minibatch training loss = 0.6728 and accuracy of 0.88\n",
      "Epoch 102, Overall loss = 0.6288 and accuracy of 0.8935\n",
      "Iteration 49980: with minibatch training loss = 0.77 and accuracy of 0.84\n",
      "Epoch 103, Overall loss = 0.5919 and accuracy of 0.9025\n",
      "Iteration 50470: with minibatch training loss = 0.5883 and accuracy of 0.86\n",
      "Epoch 104, Overall loss = 0.5659 and accuracy of 0.9066\n",
      "Iteration 50960: with minibatch training loss = 0.4564 and accuracy of 0.92\n",
      "Epoch 105, Overall loss = 0.5418 and accuracy of 0.9108\n",
      "Iteration 51450: with minibatch training loss = 0.4704 and accuracy of 0.94\n",
      "Epoch 106, Overall loss = 0.5197 and accuracy of 0.9146\n",
      "Iteration 51940: with minibatch training loss = 0.5076 and accuracy of 0.9\n",
      "Epoch 107, Overall loss = 0.5052 and accuracy of 0.9157\n",
      "Iteration 52430: with minibatch training loss = 0.5784 and accuracy of 0.88\n",
      "Epoch 108, Overall loss = 0.4902 and accuracy of 0.9183\n",
      "Iteration 52920: with minibatch training loss = 0.5632 and accuracy of 0.88\n",
      "Epoch 109, Overall loss = 0.4799 and accuracy of 0.9178\n",
      "Iteration 53410: with minibatch training loss = 0.4619 and accuracy of 0.94\n",
      "Epoch 110, Overall loss = 0.4604 and accuracy of 0.9219\n",
      "Iteration 53900: with minibatch training loss = 0.458 and accuracy of 0.88\n",
      "Epoch 111, Overall loss = 0.4497 and accuracy of 0.9231\n",
      "Iteration 54390: with minibatch training loss = 0.4344 and accuracy of 0.93\n",
      "Epoch 112, Overall loss = 0.4399 and accuracy of 0.9247\n",
      "Iteration 54880: with minibatch training loss = 0.406 and accuracy of 0.93\n",
      "Epoch 113, Overall loss = 0.4312 and accuracy of 0.9242\n",
      "Iteration 55370: with minibatch training loss = 0.315 and accuracy of 0.98\n",
      "Epoch 114, Overall loss = 0.418 and accuracy of 0.9275\n",
      "Iteration 55860: with minibatch training loss = 0.3368 and accuracy of 0.95\n",
      "Epoch 115, Overall loss = 0.4135 and accuracy of 0.9276\n",
      "Iteration 56350: with minibatch training loss = 0.4424 and accuracy of 0.92\n",
      "Epoch 116, Overall loss = 0.4064 and accuracy of 0.9285\n",
      "Iteration 56840: with minibatch training loss = 0.3023 and accuracy of 0.98\n",
      "Epoch 117, Overall loss = 0.4011 and accuracy of 0.9282\n",
      "Iteration 57330: with minibatch training loss = 0.3353 and accuracy of 0.93\n",
      "Epoch 118, Overall loss = 0.3931 and accuracy of 0.9293\n",
      "Iteration 57820: with minibatch training loss = 0.4596 and accuracy of 0.91\n",
      "Epoch 119, Overall loss = 0.3893 and accuracy of 0.9301\n",
      "Iteration 58310: with minibatch training loss = 0.4112 and accuracy of 0.92\n",
      "Epoch 120, Overall loss = 0.3851 and accuracy of 0.9307\n",
      "Iteration 58800: with minibatch training loss = 0.3357 and accuracy of 0.93\n",
      "Epoch 121, Overall loss = 0.3794 and accuracy of 0.9303\n",
      "Iteration 59290: with minibatch training loss = 0.3735 and accuracy of 0.92\n",
      "Epoch 122, Overall loss = 0.3782 and accuracy of 0.9304\n",
      "Iteration 59780: with minibatch training loss = 0.3141 and accuracy of 0.98\n",
      "Epoch 123, Overall loss = 0.3736 and accuracy of 0.9319\n",
      "Iteration 60270: with minibatch training loss = 0.3149 and accuracy of 0.96\n",
      "Epoch 124, Overall loss = 0.3723 and accuracy of 0.9307\n",
      "Iteration 60760: with minibatch training loss = 0.2992 and accuracy of 0.93\n",
      "Epoch 125, Overall loss = 0.3728 and accuracy of 0.9297\n",
      "Iteration 61250: with minibatch training loss = 0.5028 and accuracy of 0.88\n",
      "Epoch 126, Overall loss = 0.3648 and accuracy of 0.9332\n",
      "Iteration 61740: with minibatch training loss = 0.5099 and accuracy of 0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127, Overall loss = 0.3703 and accuracy of 0.9288\n",
      "Iteration 62230: with minibatch training loss = 0.2781 and accuracy of 0.96\n",
      "Epoch 128, Overall loss = 0.3658 and accuracy of 0.9311\n",
      "Iteration 62720: with minibatch training loss = 0.3434 and accuracy of 0.94\n",
      "Epoch 129, Overall loss = 0.3626 and accuracy of 0.9317\n",
      "Iteration 63210: with minibatch training loss = 0.3191 and accuracy of 0.94\n",
      "Epoch 130, Overall loss = 0.3606 and accuracy of 0.9323\n",
      "Iteration 63700: with minibatch training loss = 0.4019 and accuracy of 0.89\n",
      "Epoch 131, Overall loss = 0.3585 and accuracy of 0.9312\n",
      "Iteration 64190: with minibatch training loss = 0.3225 and accuracy of 0.94\n",
      "Epoch 132, Overall loss = 0.3517 and accuracy of 0.9347\n",
      "Iteration 64680: with minibatch training loss = 0.2694 and accuracy of 0.96\n",
      "Epoch 133, Overall loss = 0.3599 and accuracy of 0.9328\n",
      "Iteration 65170: with minibatch training loss = 0.3489 and accuracy of 0.94\n",
      "Epoch 134, Overall loss = 0.3568 and accuracy of 0.9315\n",
      "Iteration 65660: with minibatch training loss = 0.2827 and accuracy of 0.98\n",
      "Epoch 135, Overall loss = 0.3572 and accuracy of 0.9323\n",
      "Iteration 66150: with minibatch training loss = 0.3887 and accuracy of 0.93\n",
      "Epoch 136, Overall loss = 0.3577 and accuracy of 0.9321\n",
      "Iteration 66640: with minibatch training loss = 0.3314 and accuracy of 0.93\n",
      "Epoch 137, Overall loss = 0.3569 and accuracy of 0.9326\n",
      "Iteration 67130: with minibatch training loss = 0.2324 and accuracy of 0.99\n",
      "Epoch 138, Overall loss = 0.3543 and accuracy of 0.9335\n",
      "Iteration 67620: with minibatch training loss = 0.3551 and accuracy of 0.91\n",
      "Epoch 139, Overall loss = 0.3514 and accuracy of 0.9343\n",
      "Iteration 68110: with minibatch training loss = 0.2964 and accuracy of 0.95\n",
      "Epoch 140, Overall loss = 0.3514 and accuracy of 0.9349\n",
      "Iteration 68600: with minibatch training loss = 0.34 and accuracy of 0.93\n",
      "Epoch 141, Overall loss = 0.3554 and accuracy of 0.9319\n",
      "Iteration 69090: with minibatch training loss = 0.3059 and accuracy of 0.98\n",
      "Epoch 142, Overall loss = 0.3473 and accuracy of 0.9366\n",
      "Iteration 69580: with minibatch training loss = 0.3215 and accuracy of 0.93\n",
      "Epoch 143, Overall loss = 0.3497 and accuracy of 0.9341\n",
      "Iteration 70070: with minibatch training loss = 0.3608 and accuracy of 0.97\n",
      "Epoch 144, Overall loss = 0.3516 and accuracy of 0.9349\n",
      "Iteration 70560: with minibatch training loss = 0.3349 and accuracy of 0.94\n",
      "Epoch 145, Overall loss = 0.3461 and accuracy of 0.9363\n",
      "Iteration 71050: with minibatch training loss = 0.4301 and accuracy of 0.92\n",
      "Epoch 146, Overall loss = 0.3519 and accuracy of 0.9338\n",
      "Iteration 71540: with minibatch training loss = 0.2525 and accuracy of 0.97\n",
      "Epoch 147, Overall loss = 0.3455 and accuracy of 0.9378\n",
      "Iteration 72030: with minibatch training loss = 0.2586 and accuracy of 0.98\n",
      "Epoch 148, Overall loss = 0.347 and accuracy of 0.9358\n",
      "Iteration 72520: with minibatch training loss = 0.2557 and accuracy of 0.95\n",
      "Epoch 149, Overall loss = 0.3464 and accuracy of 0.9359\n",
      "Iteration 73010: with minibatch training loss = 0.2988 and accuracy of 0.94\n",
      "Epoch 150, Overall loss = 0.3469 and accuracy of 0.9369\n",
      "Iteration 73500: with minibatch training loss = 0.3411 and accuracy of 0.94\n",
      "Epoch 151, Overall loss = 0.3416 and accuracy of 0.9374\n",
      "Iteration 73990: with minibatch training loss = 0.2906 and accuracy of 0.94\n",
      "Epoch 152, Overall loss = 0.3458 and accuracy of 0.9374\n",
      "Iteration 74480: with minibatch training loss = 0.2573 and accuracy of 0.96\n",
      "Epoch 153, Overall loss = 0.3474 and accuracy of 0.9366\n",
      "Iteration 74970: with minibatch training loss = 0.3409 and accuracy of 0.94\n",
      "Epoch 154, Overall loss = 0.3451 and accuracy of 0.9372\n",
      "Iteration 75460: with minibatch training loss = 0.3953 and accuracy of 0.91\n",
      "Epoch 155, Overall loss = 0.3376 and accuracy of 0.9399\n",
      "Iteration 75950: with minibatch training loss = 0.3537 and accuracy of 0.95\n",
      "Epoch 156, Overall loss = 0.3433 and accuracy of 0.9384\n",
      "Iteration 76440: with minibatch training loss = 0.3042 and accuracy of 0.94\n",
      "Epoch 157, Overall loss = 0.3428 and accuracy of 0.9388\n",
      "Iteration 76930: with minibatch training loss = 0.2875 and accuracy of 0.97\n",
      "Epoch 158, Overall loss = 0.3372 and accuracy of 0.9409\n",
      "Iteration 77420: with minibatch training loss = 0.4082 and accuracy of 0.92\n",
      "Epoch 159, Overall loss = 0.3444 and accuracy of 0.9389\n",
      "Iteration 77910: with minibatch training loss = 0.3504 and accuracy of 0.94\n",
      "Epoch 160, Overall loss = 0.3393 and accuracy of 0.9398\n",
      "Iteration 78400: with minibatch training loss = 0.3015 and accuracy of 0.94\n",
      "Epoch 161, Overall loss = 0.3391 and accuracy of 0.9406\n",
      "Iteration 78890: with minibatch training loss = 0.3494 and accuracy of 0.94\n",
      "Epoch 162, Overall loss = 0.3346 and accuracy of 0.9425\n",
      "Iteration 79380: with minibatch training loss = 0.3065 and accuracy of 0.93\n",
      "Epoch 163, Overall loss = 0.3448 and accuracy of 0.9388\n",
      "Iteration 79870: with minibatch training loss = 0.4218 and accuracy of 0.93\n",
      "Epoch 164, Overall loss = 0.3379 and accuracy of 0.9413\n",
      "Iteration 80360: with minibatch training loss = 0.3339 and accuracy of 0.94\n",
      "Epoch 165, Overall loss = 0.3383 and accuracy of 0.9399\n",
      "Iteration 80850: with minibatch training loss = 0.3405 and accuracy of 0.96\n",
      "Epoch 166, Overall loss = 0.3389 and accuracy of 0.9403\n",
      "Iteration 81340: with minibatch training loss = 0.3475 and accuracy of 0.94\n",
      "Epoch 167, Overall loss = 0.3429 and accuracy of 0.9392\n",
      "Iteration 81830: with minibatch training loss = 0.2978 and accuracy of 0.95\n",
      "Epoch 168, Overall loss = 0.3334 and accuracy of 0.9434\n",
      "Iteration 82320: with minibatch training loss = 0.3246 and accuracy of 0.94\n",
      "Epoch 169, Overall loss = 0.3438 and accuracy of 0.9395\n",
      "Iteration 82810: with minibatch training loss = 0.3017 and accuracy of 0.96\n",
      "Epoch 170, Overall loss = 0.3421 and accuracy of 0.9404\n",
      "Iteration 83300: with minibatch training loss = 0.3207 and accuracy of 0.95\n",
      "Epoch 171, Overall loss = 0.3349 and accuracy of 0.943\n",
      "Iteration 83790: with minibatch training loss = 0.2579 and accuracy of 0.97\n",
      "Epoch 172, Overall loss = 0.3389 and accuracy of 0.9407\n",
      "Iteration 84280: with minibatch training loss = 0.3266 and accuracy of 0.94\n",
      "Epoch 173, Overall loss = 0.3367 and accuracy of 0.9427\n",
      "Iteration 84770: with minibatch training loss = 0.2484 and accuracy of 0.97\n",
      "Epoch 174, Overall loss = 0.3368 and accuracy of 0.9429\n",
      "Iteration 85260: with minibatch training loss = 0.2451 and accuracy of 0.98\n",
      "Epoch 175, Overall loss = 0.338 and accuracy of 0.9433\n",
      "Iteration 85750: with minibatch training loss = 0.3149 and accuracy of 0.94\n",
      "Epoch 176, Overall loss = 0.334 and accuracy of 0.9431\n",
      "Iteration 86240: with minibatch training loss = 0.2442 and accuracy of 0.98\n",
      "Epoch 177, Overall loss = 0.3354 and accuracy of 0.9435\n",
      "Iteration 86730: with minibatch training loss = 0.2129 and accuracy of 1\n",
      "Epoch 178, Overall loss = 0.3357 and accuracy of 0.9438\n",
      "Iteration 87220: with minibatch training loss = 0.4693 and accuracy of 0.9\n",
      "Epoch 179, Overall loss = 0.3385 and accuracy of 0.9416\n",
      "Iteration 87710: with minibatch training loss = 0.3084 and accuracy of 0.95\n",
      "Epoch 180, Overall loss = 0.3403 and accuracy of 0.9421\n",
      "Iteration 88200: with minibatch training loss = 0.251 and accuracy of 0.98\n",
      "Epoch 181, Overall loss = 0.3378 and accuracy of 0.9433\n",
      "Iteration 88690: with minibatch training loss = 0.3752 and accuracy of 0.93\n",
      "Epoch 182, Overall loss = 0.3354 and accuracy of 0.9432\n",
      "Iteration 89180: with minibatch training loss = 0.3494 and accuracy of 0.96\n",
      "Epoch 183, Overall loss = 0.3319 and accuracy of 0.9444\n",
      "Iteration 89670: with minibatch training loss = 0.2941 and accuracy of 0.94\n",
      "Epoch 184, Overall loss = 0.332 and accuracy of 0.9451\n",
      "Iteration 90160: with minibatch training loss = 0.2394 and accuracy of 0.99\n",
      "Epoch 185, Overall loss = 0.3359 and accuracy of 0.9442\n",
      "Iteration 90650: with minibatch training loss = 0.3121 and accuracy of 0.93\n",
      "Epoch 186, Overall loss = 0.3373 and accuracy of 0.9432\n",
      "Iteration 91140: with minibatch training loss = 0.3318 and accuracy of 0.93\n",
      "Epoch 187, Overall loss = 0.3324 and accuracy of 0.9442\n",
      "Iteration 91630: with minibatch training loss = 0.3619 and accuracy of 0.92\n",
      "Epoch 188, Overall loss = 0.3316 and accuracy of 0.9463\n",
      "Iteration 92120: with minibatch training loss = 0.3803 and accuracy of 0.95\n",
      "Epoch 189, Overall loss = 0.3285 and accuracy of 0.9467\n",
      "Iteration 92610: with minibatch training loss = 0.3552 and accuracy of 0.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190, Overall loss = 0.3316 and accuracy of 0.9459\n",
      "Iteration 93100: with minibatch training loss = 0.3567 and accuracy of 0.94\n",
      "Epoch 191, Overall loss = 0.3293 and accuracy of 0.9459\n",
      "Iteration 93590: with minibatch training loss = 0.3403 and accuracy of 0.93\n",
      "Epoch 192, Overall loss = 0.3337 and accuracy of 0.9448\n",
      "Iteration 94080: with minibatch training loss = 0.3005 and accuracy of 0.97\n",
      "Epoch 193, Overall loss = 0.3347 and accuracy of 0.9442\n",
      "Iteration 94570: with minibatch training loss = 0.3812 and accuracy of 0.94\n",
      "Epoch 194, Overall loss = 0.3318 and accuracy of 0.9457\n",
      "Iteration 95060: with minibatch training loss = 0.2992 and accuracy of 0.97\n",
      "Epoch 195, Overall loss = 0.3279 and accuracy of 0.9469\n",
      "Iteration 95550: with minibatch training loss = 0.4147 and accuracy of 0.92\n",
      "Epoch 196, Overall loss = 0.3305 and accuracy of 0.9459\n",
      "Iteration 96040: with minibatch training loss = 0.3088 and accuracy of 0.95\n",
      "Epoch 197, Overall loss = 0.3331 and accuracy of 0.9447\n",
      "Iteration 96530: with minibatch training loss = 0.255 and accuracy of 0.96\n",
      "Epoch 198, Overall loss = 0.3256 and accuracy of 0.9484\n",
      "Iteration 97020: with minibatch training loss = 0.3369 and accuracy of 0.95\n",
      "Epoch 199, Overall loss = 0.3306 and accuracy of 0.9456\n",
      "Iteration 97510: with minibatch training loss = 0.3624 and accuracy of 0.94\n",
      "Epoch 200, Overall loss = 0.3348 and accuracy of 0.9449\n",
      "Iteration 98000: with minibatch training loss = 0.4692 and accuracy of 0.89\n",
      "Epoch 201, Overall loss = 0.2773 and accuracy of 0.9678\n",
      "Iteration 98490: with minibatch training loss = 0.237 and accuracy of 0.99\n",
      "Epoch 202, Overall loss = 0.2535 and accuracy of 0.9768\n",
      "Iteration 98980: with minibatch training loss = 0.2438 and accuracy of 0.99\n",
      "Epoch 203, Overall loss = 0.2432 and accuracy of 0.9807\n",
      "Iteration 99470: with minibatch training loss = 0.2386 and accuracy of 0.99\n",
      "Epoch 204, Overall loss = 0.2387 and accuracy of 0.9817\n",
      "Iteration 99960: with minibatch training loss = 0.2364 and accuracy of 0.98\n",
      "Epoch 205, Overall loss = 0.2352 and accuracy of 0.9832\n",
      "Iteration 100450: with minibatch training loss = 0.2214 and accuracy of 0.99\n",
      "Epoch 206, Overall loss = 0.2319 and accuracy of 0.9835\n",
      "Iteration 100940: with minibatch training loss = 0.1936 and accuracy of 0.99\n",
      "Epoch 207, Overall loss = 0.2264 and accuracy of 0.9855\n",
      "Iteration 101430: with minibatch training loss = 0.2073 and accuracy of 0.99\n",
      "Epoch 208, Overall loss = 0.2246 and accuracy of 0.9859\n",
      "Iteration 101920: with minibatch training loss = 0.2509 and accuracy of 0.97\n",
      "Epoch 209, Overall loss = 0.2211 and accuracy of 0.9864\n",
      "Iteration 102410: with minibatch training loss = 0.2565 and accuracy of 0.97\n",
      "Epoch 210, Overall loss = 0.2189 and accuracy of 0.9868\n",
      "Iteration 102900: with minibatch training loss = 0.2106 and accuracy of 0.98\n",
      "Epoch 211, Overall loss = 0.2153 and accuracy of 0.9881\n",
      "Iteration 103390: with minibatch training loss = 0.1921 and accuracy of 1\n",
      "Epoch 212, Overall loss = 0.2154 and accuracy of 0.9875\n",
      "Iteration 103880: with minibatch training loss = 0.2301 and accuracy of 0.98\n",
      "Epoch 213, Overall loss = 0.2122 and accuracy of 0.9886\n",
      "Iteration 104370: with minibatch training loss = 0.1949 and accuracy of 0.99\n",
      "Epoch 214, Overall loss = 0.2102 and accuracy of 0.9886\n",
      "Iteration 104860: with minibatch training loss = 0.2454 and accuracy of 0.97\n",
      "Epoch 215, Overall loss = 0.2099 and accuracy of 0.9887\n",
      "Iteration 105350: with minibatch training loss = 0.3056 and accuracy of 0.94\n",
      "Epoch 216, Overall loss = 0.2077 and accuracy of 0.989\n",
      "Iteration 105840: with minibatch training loss = 0.2043 and accuracy of 0.98\n",
      "Epoch 217, Overall loss = 0.2071 and accuracy of 0.9888\n",
      "Iteration 106330: with minibatch training loss = 0.2035 and accuracy of 0.99\n",
      "Epoch 218, Overall loss = 0.2041 and accuracy of 0.9898\n",
      "Iteration 106820: with minibatch training loss = 0.2219 and accuracy of 0.96\n",
      "Epoch 219, Overall loss = 0.2034 and accuracy of 0.9898\n",
      "Iteration 107310: with minibatch training loss = 0.1784 and accuracy of 1\n",
      "Epoch 220, Overall loss = 0.1973 and accuracy of 0.9919\n",
      "Iteration 107800: with minibatch training loss = 0.2068 and accuracy of 0.99\n",
      "Epoch 221, Overall loss = 0.1991 and accuracy of 0.9912\n",
      "Iteration 108290: with minibatch training loss = 0.1896 and accuracy of 1\n",
      "Epoch 222, Overall loss = 0.1982 and accuracy of 0.9907\n",
      "Iteration 108780: with minibatch training loss = 0.1759 and accuracy of 1\n",
      "Epoch 223, Overall loss = 0.1947 and accuracy of 0.9927\n",
      "Iteration 109270: with minibatch training loss = 0.2206 and accuracy of 1\n",
      "Epoch 224, Overall loss = 0.1949 and accuracy of 0.9919\n",
      "Iteration 109760: with minibatch training loss = 0.2092 and accuracy of 0.99\n",
      "Epoch 225, Overall loss = 0.1943 and accuracy of 0.9913\n",
      "Iteration 110250: with minibatch training loss = 0.2012 and accuracy of 0.99\n",
      "Epoch 226, Overall loss = 0.1917 and accuracy of 0.9922\n",
      "Iteration 110740: with minibatch training loss = 0.1679 and accuracy of 1\n",
      "Epoch 227, Overall loss = 0.1925 and accuracy of 0.9918\n",
      "Iteration 111230: with minibatch training loss = 0.1877 and accuracy of 0.99\n",
      "Epoch 228, Overall loss = 0.1895 and accuracy of 0.9921\n",
      "Iteration 111720: with minibatch training loss = 0.2004 and accuracy of 0.99\n",
      "Epoch 229, Overall loss = 0.1883 and accuracy of 0.9923\n",
      "Iteration 112210: with minibatch training loss = 0.1785 and accuracy of 1\n",
      "Epoch 230, Overall loss = 0.185 and accuracy of 0.9933\n",
      "Iteration 112700: with minibatch training loss = 0.1882 and accuracy of 0.99\n",
      "Epoch 231, Overall loss = 0.1866 and accuracy of 0.9924\n",
      "Iteration 113190: with minibatch training loss = 0.1734 and accuracy of 1\n",
      "Epoch 232, Overall loss = 0.1846 and accuracy of 0.9928\n",
      "Iteration 113680: with minibatch training loss = 0.228 and accuracy of 0.99\n",
      "Epoch 233, Overall loss = 0.1848 and accuracy of 0.9928\n",
      "Iteration 114170: with minibatch training loss = 0.1828 and accuracy of 1\n",
      "Epoch 234, Overall loss = 0.182 and accuracy of 0.9935\n",
      "Iteration 114660: with minibatch training loss = 0.1857 and accuracy of 0.99\n",
      "Epoch 235, Overall loss = 0.1822 and accuracy of 0.9931\n",
      "Iteration 115150: with minibatch training loss = 0.1674 and accuracy of 1\n",
      "Epoch 236, Overall loss = 0.1802 and accuracy of 0.9935\n",
      "Iteration 115640: with minibatch training loss = 0.1836 and accuracy of 0.99\n",
      "Epoch 237, Overall loss = 0.1783 and accuracy of 0.994\n",
      "Iteration 116130: with minibatch training loss = 0.1616 and accuracy of 1\n",
      "Epoch 238, Overall loss = 0.1785 and accuracy of 0.9937\n",
      "Iteration 116620: with minibatch training loss = 0.1596 and accuracy of 1\n",
      "Epoch 239, Overall loss = 0.1769 and accuracy of 0.994\n",
      "Iteration 117110: with minibatch training loss = 0.1843 and accuracy of 1\n",
      "Epoch 240, Overall loss = 0.1771 and accuracy of 0.9938\n",
      "Iteration 117600: with minibatch training loss = 0.1604 and accuracy of 1\n",
      "Epoch 241, Overall loss = 0.175 and accuracy of 0.9942\n",
      "Iteration 118090: with minibatch training loss = 0.1837 and accuracy of 0.99\n",
      "Epoch 242, Overall loss = 0.1739 and accuracy of 0.9945\n",
      "Iteration 118580: with minibatch training loss = 0.1644 and accuracy of 1\n",
      "Epoch 243, Overall loss = 0.1744 and accuracy of 0.9939\n",
      "Iteration 119070: with minibatch training loss = 0.1649 and accuracy of 0.99\n",
      "Epoch 244, Overall loss = 0.1722 and accuracy of 0.9944\n",
      "Iteration 119560: with minibatch training loss = 0.1544 and accuracy of 1\n",
      "Epoch 245, Overall loss = 0.1711 and accuracy of 0.9948\n",
      "Iteration 120050: with minibatch training loss = 0.1756 and accuracy of 1\n",
      "Epoch 246, Overall loss = 0.1692 and accuracy of 0.9955\n",
      "Iteration 120540: with minibatch training loss = 0.1734 and accuracy of 1\n",
      "Epoch 247, Overall loss = 0.1686 and accuracy of 0.9952\n",
      "Iteration 121030: with minibatch training loss = 0.1788 and accuracy of 0.99\n",
      "Epoch 248, Overall loss = 0.1683 and accuracy of 0.9947\n",
      "Iteration 121520: with minibatch training loss = 0.1856 and accuracy of 0.99\n",
      "Epoch 249, Overall loss = 0.1674 and accuracy of 0.9951\n",
      "Iteration 122010: with minibatch training loss = 0.15 and accuracy of 1\n",
      "Epoch 250, Overall loss = 0.1664 and accuracy of 0.9947\n",
      "Iteration 122500: with minibatch training loss = 0.1672 and accuracy of 0.99\n",
      "Epoch 251, Overall loss = 0.1661 and accuracy of 0.9947\n",
      "Iteration 122990: with minibatch training loss = 0.1663 and accuracy of 1\n",
      "Epoch 252, Overall loss = 0.166 and accuracy of 0.9945\n",
      "Iteration 123480: with minibatch training loss = 0.15 and accuracy of 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 253, Overall loss = 0.1637 and accuracy of 0.9955\n",
      "Iteration 123970: with minibatch training loss = 0.1492 and accuracy of 1\n",
      "Epoch 254, Overall loss = 0.1637 and accuracy of 0.9949\n",
      "Iteration 124460: with minibatch training loss = 0.2039 and accuracy of 0.97\n",
      "Epoch 255, Overall loss = 0.1622 and accuracy of 0.9955\n",
      "Iteration 124950: with minibatch training loss = 0.1603 and accuracy of 1\n",
      "Epoch 256, Overall loss = 0.162 and accuracy of 0.9956\n",
      "Iteration 125440: with minibatch training loss = 0.1457 and accuracy of 1\n",
      "Epoch 257, Overall loss = 0.1607 and accuracy of 0.9953\n",
      "Iteration 125930: with minibatch training loss = 0.1486 and accuracy of 1\n",
      "Epoch 258, Overall loss = 0.1609 and accuracy of 0.9951\n",
      "Iteration 126420: with minibatch training loss = 0.1724 and accuracy of 0.99\n",
      "Epoch 259, Overall loss = 0.1592 and accuracy of 0.9958\n",
      "Iteration 126910: with minibatch training loss = 0.1426 and accuracy of 1\n",
      "Epoch 260, Overall loss = 0.1574 and accuracy of 0.996\n",
      "Iteration 127400: with minibatch training loss = 0.1618 and accuracy of 1\n",
      "Epoch 261, Overall loss = 0.1572 and accuracy of 0.996\n",
      "Iteration 127890: with minibatch training loss = 0.1649 and accuracy of 0.99\n",
      "Epoch 262, Overall loss = 0.157 and accuracy of 0.9956\n",
      "Iteration 128380: with minibatch training loss = 0.1435 and accuracy of 1\n",
      "Epoch 263, Overall loss = 0.1556 and accuracy of 0.996\n",
      "Iteration 128870: with minibatch training loss = 0.1614 and accuracy of 0.99\n",
      "Epoch 264, Overall loss = 0.1554 and accuracy of 0.996\n",
      "Iteration 129360: with minibatch training loss = 0.1473 and accuracy of 1\n",
      "Epoch 265, Overall loss = 0.1551 and accuracy of 0.9953\n",
      "Iteration 129850: with minibatch training loss = 0.147 and accuracy of 1\n",
      "Epoch 266, Overall loss = 0.1544 and accuracy of 0.9957\n",
      "Iteration 130340: with minibatch training loss = 0.1405 and accuracy of 1\n",
      "Epoch 267, Overall loss = 0.1534 and accuracy of 0.9958\n",
      "Iteration 130830: with minibatch training loss = 0.176 and accuracy of 0.98\n",
      "Epoch 268, Overall loss = 0.1515 and accuracy of 0.9964\n",
      "Iteration 131320: with minibatch training loss = 0.1701 and accuracy of 0.99\n",
      "Epoch 269, Overall loss = 0.1504 and accuracy of 0.9966\n",
      "Iteration 131810: with minibatch training loss = 0.1426 and accuracy of 1\n",
      "Epoch 270, Overall loss = 0.1504 and accuracy of 0.996\n",
      "Iteration 132300: with minibatch training loss = 0.1451 and accuracy of 1\n",
      "Epoch 271, Overall loss = 0.151 and accuracy of 0.9957\n",
      "Iteration 132790: with minibatch training loss = 0.1371 and accuracy of 1\n",
      "Epoch 272, Overall loss = 0.1493 and accuracy of 0.996\n",
      "Iteration 133280: with minibatch training loss = 0.1567 and accuracy of 0.99\n",
      "Epoch 273, Overall loss = 0.1483 and accuracy of 0.9962\n",
      "Iteration 133770: with minibatch training loss = 0.142 and accuracy of 1\n",
      "Epoch 274, Overall loss = 0.1484 and accuracy of 0.996\n",
      "Iteration 134260: with minibatch training loss = 0.1422 and accuracy of 1\n",
      "Epoch 275, Overall loss = 0.1482 and accuracy of 0.9959\n",
      "Iteration 134750: with minibatch training loss = 0.1679 and accuracy of 0.99\n",
      "Epoch 276, Overall loss = 0.1467 and accuracy of 0.9967\n",
      "Iteration 135240: with minibatch training loss = 0.1362 and accuracy of 1\n",
      "Epoch 277, Overall loss = 0.1459 and accuracy of 0.9961\n",
      "Iteration 135730: with minibatch training loss = 0.1354 and accuracy of 1\n",
      "Epoch 278, Overall loss = 0.1452 and accuracy of 0.9963\n",
      "Iteration 136220: with minibatch training loss = 0.1522 and accuracy of 0.99\n",
      "Epoch 279, Overall loss = 0.1447 and accuracy of 0.9964\n",
      "Iteration 136710: with minibatch training loss = 0.1339 and accuracy of 1\n",
      "Epoch 280, Overall loss = 0.1442 and accuracy of 0.9962\n",
      "Iteration 137200: with minibatch training loss = 0.1373 and accuracy of 1\n",
      "Epoch 281, Overall loss = 0.1433 and accuracy of 0.9964\n",
      "Iteration 137690: with minibatch training loss = 0.1304 and accuracy of 1\n",
      "Epoch 282, Overall loss = 0.1426 and accuracy of 0.9963\n",
      "Iteration 138180: with minibatch training loss = 0.1363 and accuracy of 1\n",
      "Epoch 283, Overall loss = 0.1422 and accuracy of 0.9962\n",
      "Iteration 138670: with minibatch training loss = 0.1299 and accuracy of 1\n",
      "Epoch 284, Overall loss = 0.1409 and accuracy of 0.9969\n",
      "Iteration 139160: with minibatch training loss = 0.1514 and accuracy of 1\n",
      "Epoch 285, Overall loss = 0.1415 and accuracy of 0.9962\n",
      "Iteration 139650: with minibatch training loss = 0.1295 and accuracy of 1\n",
      "Epoch 286, Overall loss = 0.1401 and accuracy of 0.9967\n",
      "Iteration 140140: with minibatch training loss = 0.1425 and accuracy of 1\n",
      "Epoch 287, Overall loss = 0.1404 and accuracy of 0.9963\n",
      "Iteration 140630: with minibatch training loss = 0.1291 and accuracy of 1\n",
      "Epoch 288, Overall loss = 0.1385 and accuracy of 0.9969\n",
      "Iteration 141120: with minibatch training loss = 0.1387 and accuracy of 0.99\n",
      "Epoch 289, Overall loss = 0.1384 and accuracy of 0.9964\n",
      "Iteration 141610: with minibatch training loss = 0.1302 and accuracy of 1\n",
      "Epoch 290, Overall loss = 0.1371 and accuracy of 0.997\n",
      "Iteration 142100: with minibatch training loss = 0.1413 and accuracy of 1\n",
      "Epoch 291, Overall loss = 0.1377 and accuracy of 0.9965\n",
      "Iteration 142590: with minibatch training loss = 0.13 and accuracy of 1\n",
      "Epoch 292, Overall loss = 0.1357 and accuracy of 0.9973\n",
      "Iteration 143080: with minibatch training loss = 0.1496 and accuracy of 0.99\n",
      "Epoch 293, Overall loss = 0.1357 and accuracy of 0.9968\n",
      "Iteration 143570: with minibatch training loss = 0.1254 and accuracy of 1\n",
      "Epoch 294, Overall loss = 0.136 and accuracy of 0.9966\n",
      "Iteration 144060: with minibatch training loss = 0.1265 and accuracy of 1\n",
      "Epoch 295, Overall loss = 0.1356 and accuracy of 0.9966\n",
      "Iteration 144550: with minibatch training loss = 0.1252 and accuracy of 1\n",
      "Epoch 296, Overall loss = 0.1334 and accuracy of 0.9971\n",
      "Iteration 145040: with minibatch training loss = 0.1315 and accuracy of 1\n",
      "Epoch 297, Overall loss = 0.1331 and accuracy of 0.997\n",
      "Iteration 145530: with minibatch training loss = 0.1422 and accuracy of 0.99\n",
      "Epoch 298, Overall loss = 0.1331 and accuracy of 0.9968\n",
      "Iteration 146020: with minibatch training loss = 0.1198 and accuracy of 1\n",
      "Epoch 299, Overall loss = 0.1321 and accuracy of 0.997\n",
      "Iteration 146510: with minibatch training loss = 0.1227 and accuracy of 1\n",
      "Epoch 300, Overall loss = 0.1323 and accuracy of 0.9968\n",
      "Iteration 147000: with minibatch training loss = 0.1217 and accuracy of 1\n",
      "Epoch 301, Overall loss = 0.1292 and accuracy of 0.9976\n",
      "Iteration 147490: with minibatch training loss = 0.144 and accuracy of 0.99\n",
      "Epoch 302, Overall loss = 0.1304 and accuracy of 0.9974\n",
      "Iteration 147980: with minibatch training loss = 0.126 and accuracy of 1\n",
      "Epoch 303, Overall loss = 0.1287 and accuracy of 0.9977\n",
      "Iteration 148470: with minibatch training loss = 0.1257 and accuracy of 1\n",
      "Epoch 304, Overall loss = 0.1285 and accuracy of 0.998\n",
      "Iteration 148960: with minibatch training loss = 0.1319 and accuracy of 1\n",
      "Epoch 305, Overall loss = 0.1285 and accuracy of 0.9978\n",
      "Iteration 149450: with minibatch training loss = 0.1228 and accuracy of 1\n",
      "Epoch 306, Overall loss = 0.128 and accuracy of 0.9981\n",
      "Iteration 149940: with minibatch training loss = 0.1207 and accuracy of 1\n",
      "Epoch 307, Overall loss = 0.1284 and accuracy of 0.9977\n",
      "Iteration 150430: with minibatch training loss = 0.133 and accuracy of 1\n",
      "Epoch 308, Overall loss = 0.1275 and accuracy of 0.9981\n",
      "Iteration 150920: with minibatch training loss = 0.1311 and accuracy of 1\n",
      "Epoch 309, Overall loss = 0.1284 and accuracy of 0.9979\n",
      "Iteration 151410: with minibatch training loss = 0.1256 and accuracy of 1\n",
      "Epoch 310, Overall loss = 0.1272 and accuracy of 0.9982\n",
      "Iteration 151900: with minibatch training loss = 0.1305 and accuracy of 1\n",
      "Epoch 311, Overall loss = 0.1272 and accuracy of 0.9983\n",
      "Iteration 152390: with minibatch training loss = 0.12 and accuracy of 1\n",
      "Epoch 312, Overall loss = 0.1276 and accuracy of 0.998\n",
      "Iteration 152880: with minibatch training loss = 0.1378 and accuracy of 1\n",
      "Epoch 313, Overall loss = 0.1275 and accuracy of 0.998\n",
      "Iteration 153370: with minibatch training loss = 0.1262 and accuracy of 1\n",
      "Epoch 314, Overall loss = 0.1271 and accuracy of 0.9983\n",
      "Iteration 153860: with minibatch training loss = 0.121 and accuracy of 1\n",
      "Epoch 315, Overall loss = 0.1269 and accuracy of 0.9984\n",
      "Iteration 154350: with minibatch training loss = 0.1211 and accuracy of 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 316, Overall loss = 0.1269 and accuracy of 0.9981\n",
      "Iteration 154840: with minibatch training loss = 0.1218 and accuracy of 1\n",
      "Epoch 317, Overall loss = 0.1265 and accuracy of 0.9984\n",
      "Iteration 155330: with minibatch training loss = 0.122 and accuracy of 1\n",
      "Epoch 318, Overall loss = 0.1259 and accuracy of 0.9986\n",
      "Iteration 155820: with minibatch training loss = 0.1279 and accuracy of 1\n",
      "Epoch 319, Overall loss = 0.1268 and accuracy of 0.9982\n",
      "Iteration 156310: with minibatch training loss = 0.12 and accuracy of 1\n",
      "Epoch 320, Overall loss = 0.1264 and accuracy of 0.9982\n",
      "Iteration 156800: with minibatch training loss = 0.1296 and accuracy of 1\n",
      "Epoch 321, Overall loss = 0.1266 and accuracy of 0.9981\n",
      "Iteration 157290: with minibatch training loss = 0.1278 and accuracy of 1\n",
      "Epoch 322, Overall loss = 0.1263 and accuracy of 0.9982\n",
      "Iteration 157780: with minibatch training loss = 0.1187 and accuracy of 1\n",
      "Epoch 323, Overall loss = 0.1263 and accuracy of 0.9981\n",
      "Iteration 158270: with minibatch training loss = 0.1404 and accuracy of 1\n",
      "Epoch 324, Overall loss = 0.1265 and accuracy of 0.9982\n",
      "Iteration 158760: with minibatch training loss = 0.143 and accuracy of 0.98\n",
      "Epoch 325, Overall loss = 0.1262 and accuracy of 0.9984\n",
      "Iteration 159250: with minibatch training loss = 0.1261 and accuracy of 1\n",
      "Epoch 326, Overall loss = 0.1251 and accuracy of 0.9987\n",
      "Iteration 159740: with minibatch training loss = 0.1171 and accuracy of 1\n",
      "Epoch 327, Overall loss = 0.1259 and accuracy of 0.9982\n",
      "Iteration 160230: with minibatch training loss = 0.1233 and accuracy of 1\n",
      "Epoch 328, Overall loss = 0.1259 and accuracy of 0.9983\n",
      "Iteration 160720: with minibatch training loss = 0.1212 and accuracy of 1\n",
      "Epoch 329, Overall loss = 0.1256 and accuracy of 0.9986\n",
      "Iteration 161210: with minibatch training loss = 0.1245 and accuracy of 1\n",
      "Epoch 330, Overall loss = 0.1258 and accuracy of 0.9984\n",
      "Iteration 161700: with minibatch training loss = 0.1224 and accuracy of 1\n",
      "Epoch 331, Overall loss = 0.1256 and accuracy of 0.9985\n",
      "Iteration 162190: with minibatch training loss = 0.1235 and accuracy of 1\n",
      "Epoch 332, Overall loss = 0.1259 and accuracy of 0.9983\n",
      "Iteration 162680: with minibatch training loss = 0.1217 and accuracy of 1\n",
      "Epoch 333, Overall loss = 0.125 and accuracy of 0.9982\n",
      "Iteration 163170: with minibatch training loss = 0.1292 and accuracy of 0.99\n",
      "Epoch 334, Overall loss = 0.1256 and accuracy of 0.9984\n",
      "Iteration 163660: with minibatch training loss = 0.1363 and accuracy of 0.99\n",
      "Epoch 335, Overall loss = 0.1257 and accuracy of 0.9982\n",
      "Iteration 164150: with minibatch training loss = 0.12 and accuracy of 1\n",
      "Epoch 336, Overall loss = 0.1258 and accuracy of 0.9982\n",
      "Iteration 164640: with minibatch training loss = 0.1182 and accuracy of 1\n",
      "Epoch 337, Overall loss = 0.125 and accuracy of 0.9986\n",
      "Iteration 165130: with minibatch training loss = 0.1206 and accuracy of 1\n",
      "Epoch 338, Overall loss = 0.1253 and accuracy of 0.9985\n",
      "Iteration 165620: with minibatch training loss = 0.1238 and accuracy of 1\n",
      "Epoch 339, Overall loss = 0.1254 and accuracy of 0.9982\n",
      "Iteration 166110: with minibatch training loss = 0.1239 and accuracy of 1\n",
      "Epoch 340, Overall loss = 0.1242 and accuracy of 0.9987\n",
      "Iteration 166600: with minibatch training loss = 0.1252 and accuracy of 1\n",
      "Epoch 341, Overall loss = 0.1252 and accuracy of 0.9983\n",
      "Iteration 167090: with minibatch training loss = 0.1167 and accuracy of 1\n",
      "Epoch 342, Overall loss = 0.1244 and accuracy of 0.9986\n",
      "Iteration 167580: with minibatch training loss = 0.132 and accuracy of 1\n",
      "Epoch 343, Overall loss = 0.1246 and accuracy of 0.9985\n",
      "Iteration 168070: with minibatch training loss = 0.1211 and accuracy of 1\n",
      "Epoch 344, Overall loss = 0.1245 and accuracy of 0.9985\n",
      "Iteration 168560: with minibatch training loss = 0.1534 and accuracy of 0.98\n",
      "Epoch 345, Overall loss = 0.1246 and accuracy of 0.9986\n",
      "Iteration 169050: with minibatch training loss = 0.1203 and accuracy of 1\n",
      "Epoch 346, Overall loss = 0.1251 and accuracy of 0.998\n",
      "Iteration 169540: with minibatch training loss = 0.127 and accuracy of 1\n",
      "Epoch 347, Overall loss = 0.1242 and accuracy of 0.9984\n",
      "Iteration 170030: with minibatch training loss = 0.1238 and accuracy of 1\n",
      "Epoch 348, Overall loss = 0.124 and accuracy of 0.9987\n",
      "Iteration 170520: with minibatch training loss = 0.1427 and accuracy of 0.99\n",
      "Epoch 349, Overall loss = 0.1242 and accuracy of 0.9986\n",
      "Iteration 171010: with minibatch training loss = 0.1175 and accuracy of 1\n",
      "Epoch 350, Overall loss = 0.1238 and accuracy of 0.9986\n",
      "Iteration 171500: with minibatch training loss = 0.1214 and accuracy of 1\n",
      "Epoch 351, Overall loss = 0.1244 and accuracy of 0.9983\n",
      "Iteration 171990: with minibatch training loss = 0.1276 and accuracy of 0.99\n",
      "Epoch 352, Overall loss = 0.1242 and accuracy of 0.9984\n",
      "Iteration 172480: with minibatch training loss = 0.1244 and accuracy of 1\n",
      "Epoch 353, Overall loss = 0.1235 and accuracy of 0.9987\n",
      "Iteration 172970: with minibatch training loss = 0.1166 and accuracy of 1\n",
      "Epoch 354, Overall loss = 0.1231 and accuracy of 0.9987\n",
      "Iteration 173460: with minibatch training loss = 0.1194 and accuracy of 1\n",
      "Epoch 355, Overall loss = 0.1235 and accuracy of 0.9986\n",
      "Iteration 173950: with minibatch training loss = 0.116 and accuracy of 1\n",
      "Epoch 356, Overall loss = 0.1235 and accuracy of 0.9986\n",
      "Iteration 174440: with minibatch training loss = 0.1222 and accuracy of 1\n",
      "Epoch 357, Overall loss = 0.1242 and accuracy of 0.9983\n",
      "Iteration 174930: with minibatch training loss = 0.1246 and accuracy of 1\n",
      "Epoch 358, Overall loss = 0.1239 and accuracy of 0.9986\n",
      "Iteration 175420: with minibatch training loss = 0.1184 and accuracy of 1\n",
      "Epoch 359, Overall loss = 0.1244 and accuracy of 0.9983\n",
      "Iteration 175910: with minibatch training loss = 0.1301 and accuracy of 1\n",
      "Epoch 360, Overall loss = 0.1239 and accuracy of 0.9983\n",
      "Iteration 176400: with minibatch training loss = 0.1202 and accuracy of 1\n",
      "Epoch 361, Overall loss = 0.124 and accuracy of 0.9985\n",
      "Iteration 176890: with minibatch training loss = 0.1158 and accuracy of 1\n",
      "Epoch 362, Overall loss = 0.1236 and accuracy of 0.9984\n",
      "Iteration 177380: with minibatch training loss = 0.123 and accuracy of 1\n",
      "Epoch 363, Overall loss = 0.1237 and accuracy of 0.9983\n",
      "Iteration 177870: with minibatch training loss = 0.1171 and accuracy of 1\n",
      "Epoch 364, Overall loss = 0.1236 and accuracy of 0.9985\n",
      "Iteration 178360: with minibatch training loss = 0.1181 and accuracy of 1\n",
      "Epoch 365, Overall loss = 0.1229 and accuracy of 0.9987\n",
      "Iteration 178850: with minibatch training loss = 0.1349 and accuracy of 0.99\n",
      "Epoch 366, Overall loss = 0.1229 and accuracy of 0.9984\n",
      "Iteration 179340: with minibatch training loss = 0.1154 and accuracy of 1\n",
      "Epoch 367, Overall loss = 0.1226 and accuracy of 0.9986\n",
      "Iteration 179830: with minibatch training loss = 0.1211 and accuracy of 1\n",
      "Epoch 368, Overall loss = 0.1225 and accuracy of 0.9988\n",
      "Iteration 180320: with minibatch training loss = 0.123 and accuracy of 1\n",
      "Epoch 369, Overall loss = 0.1225 and accuracy of 0.9989\n",
      "Iteration 180810: with minibatch training loss = 0.1157 and accuracy of 1\n",
      "Epoch 370, Overall loss = 0.1232 and accuracy of 0.9985\n",
      "Iteration 181300: with minibatch training loss = 0.1166 and accuracy of 1\n",
      "Epoch 371, Overall loss = 0.1221 and accuracy of 0.9989\n",
      "Iteration 181790: with minibatch training loss = 0.1229 and accuracy of 1\n",
      "Epoch 372, Overall loss = 0.123 and accuracy of 0.9985\n",
      "Iteration 182280: with minibatch training loss = 0.1307 and accuracy of 0.99\n",
      "Epoch 373, Overall loss = 0.1225 and accuracy of 0.9986\n",
      "Iteration 182770: with minibatch training loss = 0.116 and accuracy of 1\n",
      "Epoch 374, Overall loss = 0.1228 and accuracy of 0.9986\n",
      "Iteration 183260: with minibatch training loss = 0.1188 and accuracy of 1\n",
      "Epoch 375, Overall loss = 0.1224 and accuracy of 0.9985\n",
      "Iteration 183750: with minibatch training loss = 0.1213 and accuracy of 1\n",
      "Epoch 376, Overall loss = 0.123 and accuracy of 0.9983\n",
      "Iteration 184240: with minibatch training loss = 0.1276 and accuracy of 1\n",
      "Epoch 377, Overall loss = 0.1225 and accuracy of 0.9986\n",
      "Iteration 184730: with minibatch training loss = 0.1239 and accuracy of 1\n",
      "Epoch 378, Overall loss = 0.122 and accuracy of 0.9987\n",
      "Iteration 185220: with minibatch training loss = 0.116 and accuracy of 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 379, Overall loss = 0.122 and accuracy of 0.9987\n",
      "Iteration 185710: with minibatch training loss = 0.117 and accuracy of 1\n",
      "Epoch 380, Overall loss = 0.122 and accuracy of 0.9987\n",
      "Iteration 186200: with minibatch training loss = 0.1277 and accuracy of 0.99\n",
      "Epoch 381, Overall loss = 0.1224 and accuracy of 0.9983\n",
      "Iteration 186690: with minibatch training loss = 0.1298 and accuracy of 1\n",
      "Epoch 382, Overall loss = 0.122 and accuracy of 0.9989\n",
      "Iteration 187180: with minibatch training loss = 0.1186 and accuracy of 1\n",
      "Epoch 383, Overall loss = 0.1214 and accuracy of 0.9989\n",
      "Iteration 187670: with minibatch training loss = 0.1202 and accuracy of 1\n",
      "Epoch 384, Overall loss = 0.1221 and accuracy of 0.9986\n",
      "Iteration 188160: with minibatch training loss = 0.1162 and accuracy of 1\n",
      "Epoch 385, Overall loss = 0.1215 and accuracy of 0.9989\n",
      "Iteration 188650: with minibatch training loss = 0.116 and accuracy of 1\n",
      "Epoch 386, Overall loss = 0.1218 and accuracy of 0.9987\n",
      "Iteration 189140: with minibatch training loss = 0.1236 and accuracy of 1\n",
      "Epoch 387, Overall loss = 0.1216 and accuracy of 0.9987\n",
      "Iteration 189630: with minibatch training loss = 0.119 and accuracy of 1\n",
      "Epoch 388, Overall loss = 0.1209 and accuracy of 0.9989\n",
      "Iteration 190120: with minibatch training loss = 0.1182 and accuracy of 1\n",
      "Epoch 389, Overall loss = 0.1218 and accuracy of 0.9986\n",
      "Iteration 190610: with minibatch training loss = 0.1279 and accuracy of 1\n",
      "Epoch 390, Overall loss = 0.1218 and accuracy of 0.9986\n",
      "Iteration 191100: with minibatch training loss = 0.1299 and accuracy of 1\n",
      "Epoch 391, Overall loss = 0.121 and accuracy of 0.9989\n",
      "Iteration 191590: with minibatch training loss = 0.1283 and accuracy of 0.99\n",
      "Epoch 392, Overall loss = 0.1209 and accuracy of 0.999\n",
      "Iteration 192080: with minibatch training loss = 0.1313 and accuracy of 1\n",
      "Epoch 393, Overall loss = 0.1214 and accuracy of 0.9987\n",
      "Iteration 192570: with minibatch training loss = 0.1164 and accuracy of 1\n",
      "Epoch 394, Overall loss = 0.121 and accuracy of 0.9987\n",
      "Iteration 193060: with minibatch training loss = 0.1216 and accuracy of 1\n",
      "Epoch 395, Overall loss = 0.1209 and accuracy of 0.9987\n",
      "Iteration 193550: with minibatch training loss = 0.1203 and accuracy of 1\n",
      "Epoch 396, Overall loss = 0.1205 and accuracy of 0.9988\n",
      "Iteration 194040: with minibatch training loss = 0.1165 and accuracy of 1\n",
      "Epoch 397, Overall loss = 0.1204 and accuracy of 0.9989\n",
      "Iteration 194530: with minibatch training loss = 0.115 and accuracy of 1\n",
      "Epoch 398, Overall loss = 0.1209 and accuracy of 0.9987\n",
      "Iteration 195020: with minibatch training loss = 0.1148 and accuracy of 1\n",
      "Epoch 399, Overall loss = 0.1207 and accuracy of 0.9987\n",
      "Iteration 195510: with minibatch training loss = 0.1224 and accuracy of 1\n",
      "Epoch 400, Overall loss = 0.1209 and accuracy of 0.9987\n",
      "Iteration 196000: with minibatch training loss = 0.1175 and accuracy of 1\n",
      "Epoch 401, Overall loss = 0.1208 and accuracy of 0.9988\n",
      "Iteration 196490: with minibatch training loss = 0.1245 and accuracy of 1\n",
      "Epoch 402, Overall loss = 0.1209 and accuracy of 0.9986\n",
      "Iteration 196980: with minibatch training loss = 0.1232 and accuracy of 1\n",
      "Epoch 403, Overall loss = 0.1202 and accuracy of 0.9991\n",
      "Iteration 197470: with minibatch training loss = 0.1135 and accuracy of 1\n",
      "Epoch 404, Overall loss = 0.1206 and accuracy of 0.9987\n",
      "Iteration 197960: with minibatch training loss = 0.1164 and accuracy of 1\n",
      "Epoch 405, Overall loss = 0.1206 and accuracy of 0.9988\n",
      "Iteration 198450: with minibatch training loss = 0.1207 and accuracy of 1\n",
      "Epoch 406, Overall loss = 0.1205 and accuracy of 0.9988\n",
      "Iteration 198940: with minibatch training loss = 0.1142 and accuracy of 1\n",
      "Epoch 407, Overall loss = 0.1199 and accuracy of 0.999\n",
      "Iteration 199430: with minibatch training loss = 0.1166 and accuracy of 1\n",
      "Epoch 408, Overall loss = 0.1206 and accuracy of 0.9988\n",
      "Iteration 199920: with minibatch training loss = 0.1332 and accuracy of 0.99\n",
      "Epoch 409, Overall loss = 0.1206 and accuracy of 0.9988\n",
      "Iteration 200410: with minibatch training loss = 0.1208 and accuracy of 1\n",
      "Epoch 410, Overall loss = 0.1208 and accuracy of 0.9987\n",
      "Iteration 200900: with minibatch training loss = 0.125 and accuracy of 1\n",
      "Epoch 411, Overall loss = 0.1204 and accuracy of 0.9989\n",
      "Iteration 201390: with minibatch training loss = 0.1212 and accuracy of 1\n",
      "Epoch 412, Overall loss = 0.1203 and accuracy of 0.9987\n",
      "Iteration 201880: with minibatch training loss = 0.118 and accuracy of 1\n",
      "Epoch 413, Overall loss = 0.121 and accuracy of 0.9988\n",
      "Iteration 202370: with minibatch training loss = 0.1377 and accuracy of 0.99\n",
      "Epoch 414, Overall loss = 0.1208 and accuracy of 0.9986\n",
      "Iteration 202860: with minibatch training loss = 0.1137 and accuracy of 1\n",
      "Epoch 415, Overall loss = 0.1204 and accuracy of 0.9988\n",
      "Iteration 203350: with minibatch training loss = 0.1153 and accuracy of 1\n",
      "Epoch 416, Overall loss = 0.1207 and accuracy of 0.9988\n",
      "Iteration 203840: with minibatch training loss = 0.1146 and accuracy of 1\n",
      "Epoch 417, Overall loss = 0.1208 and accuracy of 0.9988\n",
      "Iteration 204330: with minibatch training loss = 0.1395 and accuracy of 0.99\n",
      "Epoch 418, Overall loss = 0.1207 and accuracy of 0.9989\n",
      "Iteration 204820: with minibatch training loss = 0.1194 and accuracy of 1\n",
      "Epoch 419, Overall loss = 0.1205 and accuracy of 0.9988\n",
      "Iteration 205310: with minibatch training loss = 0.115 and accuracy of 1\n",
      "Epoch 420, Overall loss = 0.1209 and accuracy of 0.9987\n",
      "Iteration 205800: with minibatch training loss = 0.1255 and accuracy of 0.99\n",
      "Epoch 421, Overall loss = 0.1209 and accuracy of 0.9985\n",
      "Iteration 206290: with minibatch training loss = 0.1162 and accuracy of 1\n",
      "Epoch 422, Overall loss = 0.1206 and accuracy of 0.9988\n",
      "Iteration 206780: with minibatch training loss = 0.1216 and accuracy of 0.99\n",
      "Epoch 423, Overall loss = 0.1205 and accuracy of 0.9988\n",
      "Iteration 207270: with minibatch training loss = 0.1186 and accuracy of 1\n",
      "Epoch 424, Overall loss = 0.1205 and accuracy of 0.9987\n",
      "Iteration 207760: with minibatch training loss = 0.1159 and accuracy of 1\n",
      "Epoch 425, Overall loss = 0.1203 and accuracy of 0.9989\n",
      "Iteration 208250: with minibatch training loss = 0.1145 and accuracy of 1\n",
      "Epoch 426, Overall loss = 0.1201 and accuracy of 0.999\n",
      "Iteration 208740: with minibatch training loss = 0.1146 and accuracy of 1\n",
      "Epoch 427, Overall loss = 0.1207 and accuracy of 0.9986\n",
      "Iteration 209230: with minibatch training loss = 0.1163 and accuracy of 1\n",
      "Epoch 428, Overall loss = 0.1204 and accuracy of 0.9986\n",
      "Iteration 209720: with minibatch training loss = 0.115 and accuracy of 1\n",
      "Epoch 429, Overall loss = 0.1203 and accuracy of 0.9988\n",
      "Iteration 210210: with minibatch training loss = 0.1227 and accuracy of 1\n",
      "Epoch 430, Overall loss = 0.1206 and accuracy of 0.9985\n",
      "Iteration 210700: with minibatch training loss = 0.1218 and accuracy of 1\n",
      "Epoch 431, Overall loss = 0.1204 and accuracy of 0.999\n",
      "Iteration 211190: with minibatch training loss = 0.1423 and accuracy of 0.99\n",
      "Epoch 432, Overall loss = 0.1205 and accuracy of 0.9989\n",
      "Iteration 211680: with minibatch training loss = 0.1155 and accuracy of 1\n",
      "Epoch 433, Overall loss = 0.1212 and accuracy of 0.9986\n",
      "Iteration 212170: with minibatch training loss = 0.1137 and accuracy of 1\n",
      "Epoch 434, Overall loss = 0.1209 and accuracy of 0.9986\n",
      "Iteration 212660: with minibatch training loss = 0.1244 and accuracy of 1\n",
      "Epoch 435, Overall loss = 0.1201 and accuracy of 0.9991\n",
      "Iteration 213150: with minibatch training loss = 0.1133 and accuracy of 1\n",
      "Epoch 436, Overall loss = 0.1201 and accuracy of 0.9989\n",
      "Iteration 213640: with minibatch training loss = 0.12 and accuracy of 1\n",
      "Epoch 437, Overall loss = 0.1205 and accuracy of 0.9989\n",
      "Iteration 214130: with minibatch training loss = 0.1156 and accuracy of 1\n",
      "Epoch 438, Overall loss = 0.1204 and accuracy of 0.9988\n",
      "Iteration 214620: with minibatch training loss = 0.1246 and accuracy of 1\n",
      "Epoch 439, Overall loss = 0.1198 and accuracy of 0.9992\n",
      "Iteration 215110: with minibatch training loss = 0.1358 and accuracy of 0.99\n",
      "Epoch 440, Overall loss = 0.1204 and accuracy of 0.9989\n",
      "Iteration 215600: with minibatch training loss = 0.1147 and accuracy of 1\n",
      "Epoch 441, Overall loss = 0.1202 and accuracy of 0.9989\n",
      "Iteration 216090: with minibatch training loss = 0.1153 and accuracy of 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 442, Overall loss = 0.1202 and accuracy of 0.999\n",
      "Iteration 216580: with minibatch training loss = 0.1207 and accuracy of 1\n",
      "Epoch 443, Overall loss = 0.12 and accuracy of 0.9991\n",
      "Iteration 217070: with minibatch training loss = 0.1146 and accuracy of 1\n",
      "Epoch 444, Overall loss = 0.1211 and accuracy of 0.9985\n",
      "Iteration 217560: with minibatch training loss = 0.1172 and accuracy of 1\n",
      "Epoch 445, Overall loss = 0.121 and accuracy of 0.9986\n",
      "Iteration 218050: with minibatch training loss = 0.1162 and accuracy of 1\n",
      "Epoch 446, Overall loss = 0.1199 and accuracy of 0.999\n",
      "Iteration 218540: with minibatch training loss = 0.1211 and accuracy of 1\n",
      "Epoch 447, Overall loss = 0.1198 and accuracy of 0.9991\n",
      "Iteration 219030: with minibatch training loss = 0.1181 and accuracy of 1\n",
      "Epoch 448, Overall loss = 0.1205 and accuracy of 0.999\n",
      "Iteration 219520: with minibatch training loss = 0.1143 and accuracy of 1\n",
      "Epoch 449, Overall loss = 0.1206 and accuracy of 0.9989\n",
      "Iteration 220010: with minibatch training loss = 0.1141 and accuracy of 1\n",
      "Epoch 450, Overall loss = 0.1199 and accuracy of 0.9987\n",
      "Iteration 220500: with minibatch training loss = 0.1267 and accuracy of 1\n",
      "Epoch 451, Overall loss = 0.1203 and accuracy of 0.999\n",
      "Iteration 220990: with minibatch training loss = 0.1237 and accuracy of 0.99\n",
      "Epoch 452, Overall loss = 0.1196 and accuracy of 0.9992\n",
      "Iteration 221480: with minibatch training loss = 0.113 and accuracy of 1\n",
      "Epoch 453, Overall loss = 0.1201 and accuracy of 0.999\n",
      "Iteration 221970: with minibatch training loss = 0.1211 and accuracy of 1\n",
      "Epoch 454, Overall loss = 0.1198 and accuracy of 0.9989\n",
      "Iteration 222460: with minibatch training loss = 0.1251 and accuracy of 1\n",
      "Epoch 455, Overall loss = 0.1205 and accuracy of 0.9987\n",
      "Iteration 222950: with minibatch training loss = 0.1149 and accuracy of 1\n",
      "Epoch 456, Overall loss = 0.1201 and accuracy of 0.9989\n",
      "Iteration 223440: with minibatch training loss = 0.1247 and accuracy of 1\n",
      "Epoch 457, Overall loss = 0.1201 and accuracy of 0.999\n",
      "Iteration 223930: with minibatch training loss = 0.1158 and accuracy of 1\n",
      "Epoch 458, Overall loss = 0.1197 and accuracy of 0.999\n",
      "Iteration 224420: with minibatch training loss = 0.1146 and accuracy of 1\n",
      "Epoch 459, Overall loss = 0.1204 and accuracy of 0.9989\n",
      "Iteration 224910: with minibatch training loss = 0.1166 and accuracy of 1\n",
      "Epoch 460, Overall loss = 0.1203 and accuracy of 0.9989\n",
      "Iteration 225400: with minibatch training loss = 0.1156 and accuracy of 1\n",
      "Epoch 461, Overall loss = 0.1202 and accuracy of 0.9991\n",
      "Iteration 225890: with minibatch training loss = 0.1171 and accuracy of 1\n",
      "Epoch 462, Overall loss = 0.1198 and accuracy of 0.999\n",
      "Iteration 226380: with minibatch training loss = 0.1143 and accuracy of 1\n",
      "Epoch 463, Overall loss = 0.1203 and accuracy of 0.999\n",
      "Iteration 226870: with minibatch training loss = 0.1149 and accuracy of 1\n",
      "Epoch 464, Overall loss = 0.1208 and accuracy of 0.9986\n",
      "Iteration 227360: with minibatch training loss = 0.1287 and accuracy of 0.99\n",
      "Epoch 465, Overall loss = 0.1207 and accuracy of 0.9987\n",
      "Iteration 227850: with minibatch training loss = 0.1175 and accuracy of 1\n",
      "Epoch 466, Overall loss = 0.1196 and accuracy of 0.999\n",
      "Iteration 228340: with minibatch training loss = 0.1156 and accuracy of 1\n",
      "Epoch 467, Overall loss = 0.1198 and accuracy of 0.9988\n",
      "Iteration 228830: with minibatch training loss = 0.1281 and accuracy of 1\n",
      "Epoch 468, Overall loss = 0.1198 and accuracy of 0.999\n",
      "Iteration 229320: with minibatch training loss = 0.1147 and accuracy of 1\n",
      "Epoch 469, Overall loss = 0.1202 and accuracy of 0.9989\n",
      "Iteration 229810: with minibatch training loss = 0.1214 and accuracy of 1\n",
      "Epoch 470, Overall loss = 0.12 and accuracy of 0.999\n",
      "Iteration 230300: with minibatch training loss = 0.1127 and accuracy of 1\n",
      "Epoch 471, Overall loss = 0.1195 and accuracy of 0.9992\n",
      "Iteration 230790: with minibatch training loss = 0.1155 and accuracy of 1\n",
      "Epoch 472, Overall loss = 0.1208 and accuracy of 0.9986\n",
      "Iteration 231280: with minibatch training loss = 0.1138 and accuracy of 1\n",
      "Epoch 473, Overall loss = 0.1204 and accuracy of 0.9987\n",
      "Iteration 231770: with minibatch training loss = 0.1204 and accuracy of 1\n",
      "Epoch 474, Overall loss = 0.1201 and accuracy of 0.9989\n",
      "Iteration 232260: with minibatch training loss = 0.1169 and accuracy of 1\n",
      "Epoch 475, Overall loss = 0.1204 and accuracy of 0.9988\n",
      "Iteration 232750: with minibatch training loss = 0.1359 and accuracy of 0.99\n",
      "Epoch 476, Overall loss = 0.1199 and accuracy of 0.9989\n",
      "Iteration 233240: with minibatch training loss = 0.1278 and accuracy of 0.99\n",
      "Epoch 477, Overall loss = 0.1203 and accuracy of 0.9988\n",
      "Iteration 233730: with minibatch training loss = 0.1149 and accuracy of 1\n",
      "Epoch 478, Overall loss = 0.1201 and accuracy of 0.9988\n",
      "Iteration 234220: with minibatch training loss = 0.1171 and accuracy of 1\n",
      "Epoch 479, Overall loss = 0.1197 and accuracy of 0.9991\n",
      "Iteration 234710: with minibatch training loss = 0.1139 and accuracy of 1\n",
      "Epoch 480, Overall loss = 0.12 and accuracy of 0.9988\n",
      "Iteration 235200: with minibatch training loss = 0.117 and accuracy of 1\n",
      "Epoch 481, Overall loss = 0.1199 and accuracy of 0.9987\n",
      "Iteration 235690: with minibatch training loss = 0.1187 and accuracy of 1\n",
      "Epoch 482, Overall loss = 0.1201 and accuracy of 0.9987\n",
      "Iteration 236180: with minibatch training loss = 0.1319 and accuracy of 0.99\n",
      "Epoch 483, Overall loss = 0.1194 and accuracy of 0.9991\n",
      "Iteration 236670: with minibatch training loss = 0.1152 and accuracy of 1\n",
      "Epoch 484, Overall loss = 0.1201 and accuracy of 0.999\n",
      "Iteration 237160: with minibatch training loss = 0.1172 and accuracy of 1\n",
      "Epoch 485, Overall loss = 0.1195 and accuracy of 0.999\n",
      "Iteration 237650: with minibatch training loss = 0.1211 and accuracy of 1\n",
      "Epoch 486, Overall loss = 0.1201 and accuracy of 0.9989\n",
      "Iteration 238140: with minibatch training loss = 0.1139 and accuracy of 1\n",
      "Epoch 487, Overall loss = 0.1203 and accuracy of 0.9984\n",
      "Iteration 238630: with minibatch training loss = 0.1182 and accuracy of 1\n",
      "Epoch 488, Overall loss = 0.1197 and accuracy of 0.9988\n",
      "Iteration 239120: with minibatch training loss = 0.1138 and accuracy of 1\n",
      "Epoch 489, Overall loss = 0.1196 and accuracy of 0.999\n",
      "Iteration 239610: with minibatch training loss = 0.1287 and accuracy of 1\n",
      "Epoch 490, Overall loss = 0.1204 and accuracy of 0.9987\n",
      "Iteration 240100: with minibatch training loss = 0.1128 and accuracy of 1\n",
      "Epoch 491, Overall loss = 0.1203 and accuracy of 0.999\n",
      "Iteration 240590: with minibatch training loss = 0.1188 and accuracy of 1\n",
      "Epoch 492, Overall loss = 0.1201 and accuracy of 0.9987\n",
      "Iteration 241080: with minibatch training loss = 0.1205 and accuracy of 1\n",
      "Epoch 493, Overall loss = 0.1199 and accuracy of 0.9988\n",
      "Iteration 241570: with minibatch training loss = 0.1141 and accuracy of 1\n",
      "Epoch 494, Overall loss = 0.1197 and accuracy of 0.9989\n",
      "Iteration 242060: with minibatch training loss = 0.1136 and accuracy of 1\n",
      "Epoch 495, Overall loss = 0.1195 and accuracy of 0.9991\n",
      "Iteration 242550: with minibatch training loss = 0.1159 and accuracy of 1\n",
      "Epoch 496, Overall loss = 0.1196 and accuracy of 0.999\n",
      "Iteration 243040: with minibatch training loss = 0.1194 and accuracy of 1\n",
      "Epoch 497, Overall loss = 0.12 and accuracy of 0.9988\n",
      "Iteration 243530: with minibatch training loss = 0.115 and accuracy of 1\n",
      "Epoch 498, Overall loss = 0.1204 and accuracy of 0.9987\n",
      "Iteration 244020: with minibatch training loss = 0.1161 and accuracy of 1\n",
      "Epoch 499, Overall loss = 0.1206 and accuracy of 0.9985\n",
      "Iteration 244510: with minibatch training loss = 0.113 and accuracy of 1\n",
      "Epoch 500, Overall loss = 0.1203 and accuracy of 0.9987\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSEShICIikf8Pd3uNgcuZiuQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fef4dad49b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fee94731048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n",
      "Epoch 1, Overall loss = 0.4644 and accuracy of 0.907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.46442643702030184, 0.90700000000000003)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feel free to play with this cell\n",
    "# This default code creates a session\n",
    "# and trains your model for 10 epochs\n",
    "# then prints the validation set accuracy\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,500,100,490,train_step,True)\n",
    "print('Validation')\n",
    "run_model(sess,y_out,mean_loss,X_val,y_val,1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 1, Overall loss = 0.1325 and accuracy of 0.9938\n",
      "Validation\n",
      "Epoch 1, Overall loss = 0.4521 and accuracy of 0.911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.45210490226745603, 0.91100000000000003)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your model here, and make sure \n",
    "# the output of this cell is the accuracy\n",
    "# of your best model on the training and val sets\n",
    "# We're looking for >= 70% accuracy on Validation\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,X_train,y_train,1,100)\n",
    "print('Validation')\n",
    "run_model(sess,y_out,mean_loss,X_val,y_val,1,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe what you did here\n",
    "In this cell you should also write an explanation of what you did, any additional features that you implemented, and any visualizations or graphs that you make in the process of training and evaluating your network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A fully convolutional network with 9 conv layers and no fc layers.\n",
    "- The structure of each conv layer is conv-bn-act.\n",
    "- Only using 1x1 and 3x3 size of convolution kernels to reduce size of parameters.\n",
    "- Using stride 2 convolution instead of max pooling.\n",
    "- Using leaky relu as activation function.\n",
    "- Using Kaiming initializer.\n",
    "- Using L2 weight decay regularization.\n",
    "- Using data augmentation including left and right flipping and random croping to avoid overfitting.\n",
    "- Using SGD with momentum to optimize the network.\n",
    "- Decaying the learning rate to 1/10, when every time the loss function is slow and concussion, \n",
    "- The final accuracy on test sets is 90.0%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set - Do this only once\n",
    "Now that we've gotten a result that we're happy with, we test our final model on the test set. This would be the score we would achieve on a competition. Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "Epoch 1, Overall loss = 0.5255 and accuracy of 0.9006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.52553233176469805, 0.90059999999999996)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Test')\n",
    "run_model(sess,y_out,mean_loss,X_test,y_test,1,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further with TensorFlow\n",
    "\n",
    "The next assignment will make heavy use of TensorFlow. You might also find it useful for your projects. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit Description\n",
    "If you implement any additional features for extra credit, clearly describe them here with pointers to any code in this or other files if applicable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
